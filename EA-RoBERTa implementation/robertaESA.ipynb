{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06efc4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate: 1.12.0\n",
      "transformers: 4.57.3\n"
     ]
    }
   ],
   "source": [
    "import accelerate, transformers\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"transformers:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe57c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Emotion-Detection'...\n",
      "remote: Enumerating objects: 57, done.\u001b[K\n",
      "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 57 (delta 9), reused 57 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (57/57), 17.60 MiB | 19.15 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n",
      "/content/Emotion-Detection/EA-RoBERTa implementation\n",
      "baseline_RoBERTa.ipynb\tdata  data_preprocess.ipynb  data.zip  reports\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/DariusCornescu/Emotion-Detection.git\n",
    "%cd Emotion-Detection/EA-RoBERTa implementation/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9183e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch , os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "from transformers import ( RobertaTokenizerFast, RobertaModel, DataCollatorWithPadding, TrainingArguments, Trainer )\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72846837",
   "metadata": {},
   "source": [
    "### ÎNCĂRCARE DATE PROCESATE\n",
    "\n",
    " 1. Încărcare DataFrames\n",
    " 2. Încărcare Label Mappings\n",
    " 3. Încărcare Config\n",
    " 4. Tokenizer\n",
    " 5. Dataset Class\n",
    " 6. Creare Datasets\n",
    " 7. Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb0d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE -- data uploaded \n",
      "   Train: 32,454\n",
      "   Val:   2,000\n",
      "   Test:  2,000\n",
      "DONE -- Labels: ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise'] -- \n",
      "DONE -- Config: model=roberta-base, max_len=128, batch=16 -- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE -- Tokenizer încărcat -- \n",
      "DONE -- Datasets create:\n",
      "   Train: 32,454\n",
      "   Val:   2,000\n",
      "   Test:  2,000\n",
      "DONE -- DataCollator configurat\n"
     ]
    }
   ],
   "source": [
    "train_df_oversampled = pd.read_csv('data/processed/train_oversampled.csv')\n",
    "val_df = pd.read_csv('data/processed/val.csv')\n",
    "test_df = pd.read_csv('data/processed/test.csv')\n",
    "\n",
    "print(\"DONE -- data uploaded \")\n",
    "print(f\"   Train: {len(train_df_oversampled):,}\")\n",
    "print(f\"   Val:   {len(val_df):,}\")\n",
    "print(f\"   Test:  {len(test_df):,}\")\n",
    "\n",
    "with open('data/processed/label_mappings.json', 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "label_to_id = mappings['label_to_id']\n",
    "id_to_label = {int(k): v for k, v in mappings['id_to_label'].items()}\n",
    "label_list = mappings['label_list']\n",
    "NUMBER_OF_LABELS = mappings['num_labels']\n",
    "\n",
    "print(f\"DONE -- Labels: {label_list} -- \")\n",
    "\n",
    "with open('data/processed/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "MODEL_NAME = config['model_name']\n",
    "MAX_LENGTH = config['max_length']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "\n",
    "print(f\"DONE -- Config: model={MODEL_NAME}, max_len={MAX_LENGTH}, batch={BATCH_SIZE} -- \")\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "print(f\"DONE -- Tokenizer încărcat -- \")\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe['label_id'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer( text, truncation=True, max_length=self.max_length, return_tensors=None )\n",
    "        \n",
    "        return { 'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask'], 'labels': label }\n",
    "\n",
    "\n",
    "train_dataset = EmotionDataset(train_df_oversampled, tokenizer, MAX_LENGTH)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"DONE -- Datasets create:\")\n",
    "print(f\"   Train: {len(train_dataset):,}\")\n",
    "print(f\"   Val:   {len(val_dataset):,}\")\n",
    "print(f\"   Test:  {len(test_dataset):,}\")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding( tokenizer=tokenizer, padding=True, return_tensors='pt' )\n",
    "print(f\"DONE -- DataCollator configurat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5163c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ESA = standard self-attention + learnable feature scaling vector S ∈ R^H.\n",
    "\n",
    "    Input:\n",
    "      E_input: [B, L, H]  (RoBERTa last_hidden_state)\n",
    "      attention_mask: [B, L] (1=real, 0=pad)\n",
    "\n",
    "    Output:\n",
    "      Z_scaled: [B, L, H]\n",
    "      attn_probs: [B, L, L]  (optional, useful for debugging/visualization)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 2, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Learned positional encoding P ∈ R[max_len, H]\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "\n",
    "        # Multi-head self-attention (Transformer-style)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Learnable scaling vector S ∈ R[H]\n",
    "        self.S = nn.Parameter(torch.ones(hidden_dim))\n",
    "\n",
    "    def forward(self, E: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        E: [B, L, H]  (e.g., roberta_outputs.last_hidden_state)\n",
    "        attention_mask: [B, L] with 1=real token, 0=pad\n",
    "        \"\"\"\n",
    "        B, L, H = E.shape\n",
    "        assert H == self.hidden_dim\n",
    "        assert L <= self.max_len, f\"Sequence length {L} exceeds max_len {self.max_len}\"\n",
    "\n",
    "        # Build positions [L] and expand to [B, L] for embedding lookup\n",
    "        positions = torch.arange(L, device=E.device).unsqueeze(0).expand(B, L)  # [B, L]\n",
    "        P = self.pos_emb(positions)  # [B, L, H]\n",
    "\n",
    "        # (1) E_input = E + P\n",
    "        E_input = E + P\n",
    "\n",
    "        # Prepare key padding mask for MHA: True means \"ignore\"\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = (attention_mask == 0)  # [B, L] boolean\n",
    "\n",
    "        # (2) Standard attention\n",
    "        Z, attn_weights = self.mha(\n",
    "            E_input, E_input, E_input,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False  # returns per-head weights (closer to attention analysis)\n",
    "        )  # Z: [B, L, H]\n",
    "\n",
    "        # (3) Emotion-specific scaling: Z_scaled = Z ⊙ S\n",
    "        Z_scaled = Z * self.S  # broadcasts [H] -> [B, L, H]\n",
    "\n",
    "        # (5) Re-add positional encoding: Z_final = Z_scaled + P\n",
    "        Z_final = Z_scaled + P\n",
    "\n",
    "        return Z_final, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "566dba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaESAClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout, id2label=None, label2id=None, dense_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "        cfg = self.roberta.config\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        self.esa_layer = ESALayer(\n",
    "            hidden_dim=hidden_size, \n",
    "            num_heads=cfg.num_attention_heads, \n",
    "            max_len=cfg.max_position_embeddings\n",
    "        )\n",
    "\n",
    "        self.pre_dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(2 *hidden_size, dense_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(dense_dim, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        if id2label is not None:\n",
    "            self.roberta.config.id2label = id2label\n",
    "        if label2id is not None:\n",
    "            self.roberta.config.label2id = label2id\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        E = outputs.last_hidden_state  # [B, L, H]\n",
    "        Z_final, esa_attn_probs = self.esa_layer(E, attention_mask=attention_mask)\n",
    "\n",
    "        # CLS pooling\n",
    "        cls_pool = Z_final[:, 0, :] \n",
    "\n",
    "        # Mean pooling (masked)\n",
    "        mask = attention_mask.unsqueeze(-1)  # [B, L, 1]\n",
    "        sum_pool = (Z_final * mask).sum(dim=1)\n",
    "        len_pool = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        mean_pool = sum_pool / len_pool      # [B, H]\n",
    "\n",
    "        # Concatenate pooling outputs\n",
    "        pooled = torch.cat([cls_pool, mean_pool], dim=-1)  # [B, 2H]\n",
    "\n",
    "        X = self.dense(pooled)\n",
    "\n",
    "        logits = self.classifier(X)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c84b9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE -- Model încărcat pe cpu\n",
      "   Total parametri: 127,798,534\n",
      "   Parametri antrenabili: 127,798,534\n"
     ]
    }
   ],
   "source": [
    "BASELINE_CONFIGURATION = {\n",
    "    'model_name': 'roberta-base',\n",
    "    'learning_rate': 1e-5,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 10,\n",
    "    'dropout': 0.3,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_length': 128,\n",
    "    'num_labels': 6\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "id2label = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'love', 4: 'sadness', 5: 'surprise'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "os.makedirs('models/baseline_roberta', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "model = RobertaESAClassifier(\n",
    "    model_name=BASELINE_CONFIGURATION[\"model_name\"],\n",
    "    num_labels=BASELINE_CONFIGURATION[\"num_labels\"],\n",
    "    dropout=BASELINE_CONFIGURATION[\"dropout\"],\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nDONE -- Model încărcat pe {device}\")\n",
    "print(f\"   Total parametri: {total_params:,}\")\n",
    "print(f\"   Parametri antrenabili: {trainable_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e05a3874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 6])\n",
      "Loss: 1.8087496757507324\n",
      "Backward pass OK\n"
     ]
    }
   ],
   "source": [
    "model.train() \n",
    "\n",
    "# take ONE small batch\n",
    "batch = next(iter(DataLoader(train_dataset, batch_size=2, collate_fn=data_collator)))\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "# forward\n",
    "out = model(**batch)\n",
    "\n",
    "print(\"Logits shape:\", out.logits.shape)  # should be [2, 6]\n",
    "print(\"Loss:\", out.loss.item())\n",
    "\n",
    "# backward (gradient check)\n",
    "out.loss.backward()\n",
    "\n",
    "print(\"Backward pass OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d9541",
   "metadata": {},
   "source": [
    "Although the architecture diagram includes a Softmax layer, in our implementation Softmax is applied implicitly within the CrossEntropyLoss during training and explicitly only at inference time to obtain emotion probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

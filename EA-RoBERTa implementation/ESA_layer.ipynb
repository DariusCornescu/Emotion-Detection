{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce74352",
   "metadata": {},
   "source": [
    "# Emotion-Specific Attention (ESA)\n",
    "\n",
    "Emotion-Specific Attention (ESA) is an attention-based layer introduced in the paper\n",
    "*Emotion-Aware RoBERTa enhanced with emotion-specific attention and TF-IDF gating*.\n",
    "\n",
    "ESA is designed to **refine contextual token representations by amplifying\n",
    "emotion-relevant latent features**, without modifying the internal attention\n",
    "mechanism of RoBERTa.\n",
    "\n",
    "Key ideas:\n",
    "- ESA operates **after** RoBERTa has produced contextual embeddings\n",
    "- It uses **standard self-attention**\n",
    "- Emotional awareness is introduced via a **learnable feature-scaling vector**\n",
    "- ESA does **not** assign importance directly to tokens, but to embedding dimensions\n",
    "\n",
    "ESA is task-specific and trained end-to-end via the final emotion classification loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "875fa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46af0b",
   "metadata": {},
   "source": [
    "## Dummy input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad23da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Sequence length: 16\n",
      "Hidden dimension: 768\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 16\n",
    "HIDDEN_DIM = 768   \n",
    "\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Sequence length:\", SEQ_LEN)\n",
    "print(\"Hidden dimension:\", HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d313da86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E shape: torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# Simulated RoBERTa output embeddings\n",
    "# Shape: [B, L, H]\n",
    "E = torch.randn(BATCH_SIZE, SEQ_LEN, HIDDEN_DIM)\n",
    "\n",
    "print(\"E shape:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ac4cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask shape: torch.Size([2, 16])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Attention mask: 1 = real token, 0 = padding\n",
    "attention_mask = torch.tensor([\n",
    "    [1]*12 + [0]*4,     # 12 real tokens + 4 pads\n",
    "    [1]*16              # all real\n",
    "])\n",
    "\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd83f0",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "We are now ready to implement the ESA layer itself.\n",
    "\n",
    "Next we will:\n",
    "1. Add positional encoding\n",
    "2. Implement standard scaled dot-product attention\n",
    "3. Introduce the learnable emotion-scaling vector S ∈ ℝᴴ\n",
    "4. Apply feature-wise scaling\n",
    "5. Re-add positional encoding\n",
    "6. Verify shapes and gradients\n",
    "\n",
    "No TF-IDF gating, no training loop, no optimization yet.\n",
    "\n",
    "## 1. Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13621542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_demo shape: torch.Size([16, 768])\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL / EDUCATIONAL ONLY\n",
    "# This demonstrates how positional encodings are typically constructed in vanilla Transformers.\n",
    "# We will NOT apply this to RoBERTa outputs (last_hidden_state), because RoBERTa already includes\n",
    "# learned positional embeddings internally.\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_dim, 2) * (-torch.log(torch.tensor(10000.0)) / hidden_dim)\n",
    "        )\n",
    "\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, H]\n",
    "        seq_len = x.size(1)\n",
    "        return self.pe[:seq_len]  # [L, H]\n",
    "\n",
    "\n",
    "# Quick demo (again: NOT used later)\n",
    "pos_enc = SinusoidalPositionalEncoding(hidden_dim=HIDDEN_DIM, max_len=SEQ_LEN)\n",
    "P_demo = pos_enc(E)  # [L, H]\n",
    "print(\"P_demo shape:\", P_demo.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63cd60",
   "metadata": {},
   "source": [
    "## Why the paper writes `E_input = E + P`, and what we do in practice\n",
    "\n",
    "In the ESA formulation, the paper explicitly adds positional encodings:\n",
    "\n",
    "`E_input = E + P`\n",
    "\n",
    "This is a standard description used in Transformer attention, where token embeddings `E`\n",
    "need positional information `P` because attention alone is order-invariant.\n",
    "\n",
    "However, in our implementation `E` comes from:\n",
    "\n",
    "`E = roberta_outputs.last_hidden_state`\n",
    "\n",
    "RoBERTa already adds **learned positional embeddings** inside its embedding layer\n",
    "(before the transformer blocks). Therefore, `last_hidden_state` already contains\n",
    "positional information.\n",
    "\n",
    "So in practice, for ESA built *on top of RoBERTa outputs*, we do:\n",
    "\n",
    "- `E_input = E`\n",
    "\n",
    "and we do **not** add an additional external positional encoding, to avoid double-counting position.\n",
    "\n",
    "This notebook keeps an example positional encoding implementation above for reference, but it is not used in the ESA forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7664006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_input shape: torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# This is the actual input to ESA in our setup:\n",
    "E_input = E  # RoBERTa already injected positional information\n",
    "print(\"E_input shape:\", E_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6e682",
   "metadata": {},
   "source": [
    "## 2. Self-attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f47034de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal single-head self-attention:\n",
    "    Input:  E_input  [B, L, H]\n",
    "    Mask:   attention_mask [B, L] with 1=real token, 0=pad\n",
    "    Output: Z [B, L, H] and attn_probs [B, L, L]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Linear projections\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, E_input: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        B, L, H = E_input.size()\n",
    "        assert H == self.hidden_dim, f\"Expected hidden_dim={self.hidden_dim}, got H={H}\"\n",
    "\n",
    "        # 1. Project to Q, K, V\n",
    "        Q = self.Wq(E_input)  # [B, L, H]\n",
    "        K = self.Wk(E_input)  # [B, L, H]\n",
    "        V = self.Wv(E_input)  # [B, L, H]\n",
    "\n",
    "        # 2. Scaled dot-product attention scores\n",
    "        # scores[b] = Q[b] @ K[b].T => [L, L]\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / (H ** 0.5)  # [B, L, L]\n",
    "\n",
    "        # 3. Apply attention mask (if provided)\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask: [B, L] -> key_mask: [B, 1, L] brodcast over query positions\n",
    "            key_mask = attention_mask.unsqueeze(1)  # [B, 1, L]\n",
    "            scores = scores.masked_fill(key_mask == 0, float('-inf'))\n",
    "\n",
    "        # 4. Softmax to get attention probabilities\n",
    "        attn_probs = F.softmax(scores, dim=-1)  # [B, L, L]\n",
    "\n",
    "        # If a row in attn_probs is all -inf (due to masking), softmax gives NaNs.\n",
    "        # This should not happen if each sample has at least 1 real token.\n",
    "        if torch.isnan(attn_probs).any():\n",
    "            raise ValueError(\"NaNs detected in attn_probs. Check attention_mask and padding.\")\n",
    "\n",
    "        # 5. Weighted sum of values\n",
    "        Z = torch.matmul(attn_probs, V)  # [B, L, H]\n",
    "\n",
    "        return Z, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9a938ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: torch.Size([2, 16, 768])\n",
      "attn_probs shape: torch.Size([2, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "attn = ScaledDotProductSelfAttention(hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "Z, attn_probs = attn(E_input, attention_mask=attention_mask)\n",
    "\n",
    "print(\"Z shape:\", Z.shape)                 # [B, L, H]\n",
    "print(\"attn_probs shape:\", attn_probs.shape)  # [B, L, L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18758576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask (sample 0): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "Sum of attention mass going to PAD keys (sample 0): 10.67730712890625\n"
     ]
    }
   ],
   "source": [
    "# For sample 0, tokens at positions 4 and 5 are PAD (mask=0).\n",
    "# So attention probabilities toward those positions should be ~0.\n",
    "print(\"Attention mask (sample 0):\", attention_mask[0].tolist())\n",
    "print(\"Sum of attention mass going to PAD keys (sample 0):\",\n",
    "      attn_probs[0, :, 4:].sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e6393",
   "metadata": {},
   "source": [
    "## 3. learnable emotion-scaling vector S ∈ ℝᴴ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fc43014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ESA = standard self-attention + learnable feature scaling vector S ∈ R^H.\n",
    "\n",
    "    Input:\n",
    "      E_input: [B, L, H]  (RoBERTa last_hidden_state)\n",
    "      attention_mask: [B, L] (1=real, 0=pad)\n",
    "\n",
    "    Output:\n",
    "      Z_scaled: [B, L, H]\n",
    "      attn_probs: [B, L, L]  (optional, useful for debugging/visualization)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 2, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Learned positional encoding P ∈ R[max_len, H]\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "\n",
    "        # Multi-head self-attention (Transformer-style)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Learnable scaling vector S ∈ R[H]\n",
    "        self.S = nn.Parameter(torch.ones(hidden_dim))\n",
    "\n",
    "    def forward(self, E: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        E: [B, L, H]  (e.g., roberta_outputs.last_hidden_state)\n",
    "        attention_mask: [B, L] with 1=real token, 0=pad\n",
    "        \"\"\"\n",
    "        B, L, H = E.shape\n",
    "        assert H == self.hidden_dim\n",
    "        assert L <= self.max_len, f\"Sequence length {L} exceeds max_len {self.max_len}\"\n",
    "\n",
    "        # Build positions [L] and expand to [B, L] for embedding lookup\n",
    "        positions = torch.arange(L, device=E.device).unsqueeze(0).expand(B, L)  # [B, L]\n",
    "        P = self.pos_emb(positions)  # [B, L, H]\n",
    "\n",
    "        # (1) E_input = E + P\n",
    "        E_input = E + P\n",
    "\n",
    "        # Prepare key padding mask for MHA: True means \"ignore\"\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = (attention_mask == 0)  # [B, L] boolean\n",
    "\n",
    "        # (2) Standard attention\n",
    "        Z, attn_weights = self.mha(\n",
    "            E_input, E_input, E_input,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False  # returns per-head weights (closer to attention analysis)\n",
    "        )  # Z: [B, L, H]\n",
    "\n",
    "        # (3) Emotion-specific scaling: Z_scaled = Z ⊙ S\n",
    "        Z_scaled = Z * self.S  # broadcasts [H] -> [B, L, H]\n",
    "\n",
    "        # (5) Re-add positional encoding: Z_final = Z_scaled + P\n",
    "        Z_final = Z_scaled + P\n",
    "\n",
    "        return Z_final, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b9d8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_final: torch.Size([2, 16, 768])\n",
      "attn_w: torch.Size([2, 12, 16, 16])\n",
      "S: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "esa = ESALayer(hidden_dim=HIDDEN_DIM, num_heads=12, max_len=514)\n",
    "Z_final, attn_w = esa(E_input, attention_mask=attention_mask)\n",
    "\n",
    "print(\"Z_final:\", Z_final.shape)        # [B, L, H]\n",
    "print(\"attn_w:\", attn_w.shape)          # [B, num_heads, L, L]\n",
    "print(\"S:\", esa.S.shape)                # [H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d27f618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S grad exists: True\n",
      "pos_emb grad exists: True\n",
      "pos_emb grad norm: 0.009953795000910759\n"
     ]
    }
   ],
   "source": [
    "esa = ESALayer(hidden_dim=HIDDEN_DIM, num_heads=12, max_len=514)\n",
    "esa.train()\n",
    "Z_final, _ = esa(E_input, attention_mask=attention_mask)\n",
    "\n",
    "loss = Z_final.mean()\n",
    "esa.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "\n",
    "print(\"S grad exists:\", esa.S.grad is not None)\n",
    "print(\"pos_emb grad exists:\", esa.pos_emb.weight.grad is not None)\n",
    "if esa.pos_emb.weight.grad is not None:\n",
    "    print(\"pos_emb grad norm:\", esa.pos_emb.weight.grad.norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3df5cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_emb grad exists: False\n"
     ]
    }
   ],
   "source": [
    "print(\"pos_emb grad exists:\", esa.pos_emb.weight.grad is not None)\n",
    "if esa.pos_emb.weight.grad is not None:\n",
    "    print(\"pos_emb grad norm:\", esa.pos_emb.weight.grad.norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44582a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad positions sample 0: [12, 13, 14, 15]\n",
      "Total attention mass going to PAD keys (sample 0): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Identify pad positions for sample 0\n",
    "pad_positions = (attention_mask[0] == 0).nonzero(as_tuple=True)[0]\n",
    "print(\"Pad positions sample 0:\", pad_positions.tolist())\n",
    "\n",
    "if len(pad_positions) > 0:\n",
    "    pad_attention_mass = attn_w[0, :, :, pad_positions].sum().item()\n",
    "    print(\"Total attention mass going to PAD keys (sample 0):\", pad_attention_mass)\n",
    "else:\n",
    "    print(\"No padding in sample 0 to test masking.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

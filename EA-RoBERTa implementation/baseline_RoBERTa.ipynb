{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06efc4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate: 1.12.0\n",
      "transformers: 4.57.3\n"
     ]
    }
   ],
   "source": [
    "import accelerate, transformers\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"transformers:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9183e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch , os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "from transformers import ( RobertaTokenizerFast, RobertaForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72846837",
   "metadata": {},
   "source": [
    "### ÎNCĂRCARE DATE PROCESATE\n",
    "\n",
    " 1. Încărcare DataFrames\n",
    " 2. Încărcare Label Mappings\n",
    " 3. Încărcare Config\n",
    " 4. Tokenizer\n",
    " 5. Dataset Class\n",
    " 6. Creare Datasets\n",
    " 7. Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE -- data uploaded \n",
      "   Train: 32,454\n",
      "   Val:   2,000\n",
      "   Test:  2,000\n",
      "DONE -- Labels: ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise'] -- \n",
      "DONE -- Config: model=roberta-base, max_len=128, batch=16 -- \n",
      " DONE -- Tokenizer încărcat -- \n",
      " DONE -- Datasets create:\n",
      "   Train: 32,454\n",
      "   Val:   2,000\n",
      "   Test:  2,000\n",
      "DONE -- DataCollator configurat\n"
     ]
    }
   ],
   "source": [
    "train_df_oversampled = pd.read_csv('data/processed/train_oversampled.csv')\n",
    "val_df = pd.read_csv('data/processed/val.csv')\n",
    "test_df = pd.read_csv('data/processed/test.csv')\n",
    "\n",
    "print(\"DONE -- data uploaded \")\n",
    "print(f\"   Train: {len(train_df_oversampled):,}\")\n",
    "print(f\"   Val:   {len(val_df):,}\")\n",
    "print(f\"   Test:  {len(test_df):,}\")\n",
    "\n",
    "with open('data/processed/label_mappings.json', 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "label_to_id = mappings['label_to_id']\n",
    "id_to_label = {int(k): v for k, v in mappings['id_to_label'].items()}\n",
    "label_list = mappings['label_list']\n",
    "NUMBER_OF_LABELS = mappings['num_labels']\n",
    "\n",
    "print(f\"DONE -- Labels: {label_list} -- \")\n",
    "\n",
    "with open('data/processed/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "MODEL_NAME = config['model_name']\n",
    "MAX_LENGTH = config['max_length']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "\n",
    "print(f\"DONE -- Config: model={MODEL_NAME}, max_len={MAX_LENGTH}, batch={BATCH_SIZE} -- \")\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "print(f\"DONE -- Tokenizer încărcat -- \")\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe['label_id'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer( text, truncation=True, max_length=self.max_length, return_tensors=None )\n",
    "        \n",
    "        return { 'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask'], 'labels': label }\n",
    "\n",
    "\n",
    "train_dataset = EmotionDataset(train_df_oversampled, tokenizer, MAX_LENGTH)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"DONE -- Datasets create:\")\n",
    "print(f\"   Train: {len(train_dataset):,}\")\n",
    "print(f\"   Val:   {len(val_dataset):,}\")\n",
    "print(f\"   Test:  {len(test_dataset):,}\")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding( tokenizer=tokenizer, padding=True, return_tensors='pt' )\n",
    "print(f\"DONE -- DataCollator configurat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c84b9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE -- Model încărcat pe cpu\n",
      "   Total parametri: 124,650,246\n",
      "   Parametri antrenabili: 124,650,246\n"
     ]
    }
   ],
   "source": [
    "BASELINE_CONFIGURATION = {\n",
    "    'model_name': 'roberta-base',\n",
    "    'learning_rate': 1e-5,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 10,\n",
    "    'dropout': 0.3,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_length': 128,\n",
    "    'num_labels': 6\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "id2label = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'love', 4: 'sadness', 5: 'surprise'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "os.makedirs('models/baseline_roberta', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    BASELINE_CONFIGURATION['model_name'],\n",
    "    num_labels=BASELINE_CONFIGURATION['num_labels'],\n",
    "    hidden_dropout_prob=BASELINE_CONFIGURATION['dropout'],\n",
    "    attention_probs_dropout_prob=BASELINE_CONFIGURATION['dropout'],\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nDONE -- Model încărcat pe {device}\")\n",
    "print(f\"   Total parametri: {total_params:,}\")\n",
    "print(f\"   Parametri antrenabili: {trainable_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26b3aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculează metrici pentru evaluare\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support( labels, predictions, average='weighted' )\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support( labels, predictions, average='macro' )\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    \n",
    "    return { 'accuracy': accuracy, 'precision_weighted': precision, 'recall_weighted': recall, 'f1_weighted': f1,\n",
    "        'precision_macro': precision_macro, 'recall_macro': recall_macro, 'f1_macro': f1_macro, 'mcc': mcc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da6bdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/baseline_roberta',\n",
    "\n",
    "    # Training params\n",
    "    num_train_epochs=BASELINE_CONFIGURATION['num_epochs'],\n",
    "    per_device_train_batch_size=BASELINE_CONFIGURATION['batch_size'],\n",
    "    per_device_eval_batch_size=BASELINE_CONFIGURATION['batch_size'],\n",
    "    learning_rate=BASELINE_CONFIGURATION['learning_rate'],\n",
    "    weight_decay=BASELINE_CONFIGURATION['weight_decay'],\n",
    "    warmup_ratio=BASELINE_CONFIGURATION['warmup_ratio'],\n",
    "\n",
    "    # Evaluation & Saving\n",
    "    eval_strategy='epoch',      \n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_weighted',\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # Logging\n",
    "    logging_dir='./logs/baseline_roberta',\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "\n",
    "    # Optimizări\n",
    "    use_cpu=True,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,   # Windows\n",
    "\n",
    "    # Altele\n",
    "    report_to='none',\n",
    "    seed=42,\n",
    "\n",
    "    # Salvare\n",
    "    save_total_limit=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb3afecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE -- Training info:\n",
      "   Train samples: 32,454\n",
      "   Val samples: 2,000\n",
      "   Batches per epoch: 2028\n",
      "   Total steps: 20280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dariu\\AppData\\Local\\Temp\\ipykernel_43512\\3204235106.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics )\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics )\n",
    "\n",
    "print(f\"\\nDONE -- Training info:\")\n",
    "print(f\"   Train samples: {len(train_dataset):,}\")\n",
    "print(f\"   Val samples: {len(val_dataset):,}\")\n",
    "print(f\"   Batches per epoch: {len(train_dataset) // BASELINE_CONFIGURATION['batch_size']}\")\n",
    "print(f\"   Total steps: {len(train_dataset) // BASELINE_CONFIGURATION['batch_size'] * BASELINE_CONFIGURATION['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antrenare\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" Antrenare BASELINE RoBERTa\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Epochs: {BASELINE_CONFIGURATION['num_epochs']}\")\n",
    "print(f\"   Batch size: {BASELINE_CONFIGURATION['batch_size']}\")\n",
    "print(f\"   Learning rate: {BASELINE_CONFIGURATION['learning_rate']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nDONE -- Antrenare completă!\")\n",
    "print(f\"   Training Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Training Time: {train_result.metrics['train_runtime']:.1f}s\")\n",
    "\n",
    "# Salvare Model\n",
    "trainer.save_model('./models/baseline_roberta')\n",
    "tokenizer.save_pretrained('./models/baseline_roberta')\n",
    "\n",
    "print(\"DONE -- Model salvat în ./models/baseline_roberta/\")\n",
    "\n",
    "# Evaluare pe Validation Set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evalare pe validation set\")\n",
    "print(\"=\" * 60)\n",
    "val_results = trainer.evaluate(val_dataset)\n",
    "\n",
    "print(\"\\nRezultate Validation:\")\n",
    "for key, value in val_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Evaluare pe Test Set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evalare pe test set\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\nRezultate Test:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Predicții detaliate pe Test Set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analiza detaliată pe test set\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = np.array([test_dataset[i]['labels'] for i in range(len(test_dataset))])\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=label_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee106a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers datasets torch scikit-learn pandas matplotlib seaborn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca21489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os \n",
    "import re\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, matthews_corrcoef\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711a2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully , having shape: (20000, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_emotional_data(file_path, sep=';'):\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(sep, 1)  \n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "\n",
    "            texts.append(parts[0].strip())\n",
    "            labels.append(parts[1].strip())\n",
    "\n",
    "    df = pd.DataFrame({'text': texts, 'label_str': labels})\n",
    "\n",
    "    label_map = {lab: i for i, lab in enumerate(sorted(df['label_str'].unique()))}\n",
    "    df['label'] = df['label_str'].map(label_map).astype(int)\n",
    "\n",
    "    return df, label_map\n",
    "\n",
    "try:\n",
    "    train_df, train_label_map = load_emotional_data(r\"../database_used_by_paper/train.txt\")\n",
    "    val_df, _ = load_emotional_data(r\"../database_used_by_paper/val.txt\")\n",
    "    test_df, _ = load_emotional_data(r\"../database_used_by_paper/test.txt\")\n",
    "\n",
    "    dataframe = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "    print(\"Data loaded successfully , having shape:\", dataframe.shape)\n",
    "except Exception as e:\n",
    "    print(\"Error loading data:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 20000\n",
      "\n",
      "Coloane: ['text', 'label_str', 'label']\n",
      "\n",
      "Primele 5 exemple:\n",
      "                                                text label_str  label\n",
      "0                            i didnt feel humiliated   sadness      4\n",
      "1  i can go from feeling so hopeless to so damned...   sadness      4\n",
      "2   im grabbing a minute to post i feel greedy wrong     anger      0\n",
      "3  i am ever feeling nostalgic about the fireplac...      love      3\n",
      "4                               i am feeling grouchy     anger      0\n",
      "\n",
      "Verificare valori lipsa pe coloane:\n",
      "text         0\n",
      "label_str    0\n",
      "label        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal samples: {len(dataframe)}\")\n",
    "print(f\"\\nColoane: {dataframe.columns.tolist()}\")\n",
    "print(f\"\\nPrimele 5 exemple:\")\n",
    "print(dataframe.head())\n",
    "print(\"\\nVerificare valori lipsa pe coloane:\")\n",
    "print(dataframe.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e30a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Distributia claselor din setul complet de date:\n",
      "==================================================\n",
      "Clasa: joy | NumÄƒr de exemple: 6761 | Procentaj: 33.80%\n",
      "Clasa: sadness | NumÄƒr de exemple: 5797 | Procentaj: 28.98%\n",
      "Clasa: anger | NumÄƒr de exemple: 2709 | Procentaj: 13.54%\n",
      "Clasa: fear | NumÄƒr de exemple: 2373 | Procentaj: 11.87%\n",
      "Clasa: love | NumÄƒr de exemple: 1641 | Procentaj: 8.21%\n",
      "Clasa: surprise | NumÄƒr de exemple: 719 | Procentaj: 3.60%\n",
      "\n",
      "Statistici despre lungimea textelor:\n",
      "        text_length    word_count\n",
      "count  20000.000000  20000.000000\n",
      "mean      96.670050     19.135050\n",
      "std       55.777923     10.972016\n",
      "min        7.000000      2.000000\n",
      "25%       53.000000     11.000000\n",
      "50%       86.000000     17.000000\n",
      "75%      129.000000     25.000000\n",
      "max      300.000000     66.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Distributia claselor din setul complet de date:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class_distribution = dataframe['label_str'].value_counts()\n",
    "class_percentages = (class_distribution / len(dataframe)) * 100\n",
    "\n",
    "for label, count in class_distribution.items():\n",
    "    percentage = class_percentages[label]\n",
    "    print(f\"Clasa: {label} | NumÄƒr de exemple: {count} | Procentaj: {percentage:.2f}%\")\n",
    "\n",
    "dataframe['text_length'] = dataframe['text'].apply(len)\n",
    "dataframe['word_count'] = dataframe['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nStatistici despre lungimea textelor:\")\n",
    "print(dataframe[['text_length', 'word_count']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Text Cleaning:\n",
      "------------------------------------------------------------\n",
      "Original: I'm SO happy!!! ðŸ˜Š https://example.com @friend #blessed\n",
      "Cleaned:  I'm SO happy!! blessed\n",
      "\n",
      "Original: This is    terrible...   I hate it!!!\n",
      "Cleaned:  This is terrible.. I hate it!!\n",
      "\n",
      "Original: Why would you do that?? @someone #confused\n",
      "Cleaned:  Why would you do that?? confused\n",
      "\n",
      "Original: Normal text with some punctuation, like this.\n",
      "Cleaned:  Normal text with some punctuation, like this.\n",
      "\n",
      "Original: ANGRY!!! ðŸ˜¡ðŸ˜¡ðŸ˜¡ I can't believe this @#$%^\n",
      "Cleaned:  ANGRY!! I can't believe this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    CurÄƒÈ›Äƒ textul conform specificaÈ›iilor din paper:\n",
    "    - EliminÄƒ URL-uri, mentions, caractere speciale\n",
    "    - PÄƒstreazÄƒ punctuaÈ›ia emoÈ›ionalÄƒ (!, ?, ...)\n",
    "    - NormalizeazÄƒ whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # EliminÄƒm URL-uri\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # EliminÄƒm mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # EliminÄƒm hashtags dar pÄƒstreazÄƒ textul (ex: #happy -> happy)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # EliminÄƒm caractere speciale DAR pÄƒstreazÄƒ punctuaÈ›ia emoÈ›ionalÄƒ\n",
    "    # PÄƒstrÄƒm: litere, cifre, spaÈ›ii, È™i punctuaÈ›ia emoÈ›ionalÄƒ (! ? . , ' \")\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s!?.,;:'\\\"\\-]\", '', text)\n",
    "    \n",
    "    # NormalizeazÄƒ punctuaÈ›ia repetatÄƒ (!!!! -> !!)\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # EliminÄƒm whitespace excesiv\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "test_texts = [\n",
    "    \"I'm SO happy!!! ðŸ˜Š https://example.com @friend #blessed\",\n",
    "    \"This is    terrible...   I hate it!!!\",\n",
    "    \"Why would you do that?? @someone #confused\",\n",
    "    \"Normal text with some punctuation, like this.\",\n",
    "    \"ANGRY!!! ðŸ˜¡ðŸ˜¡ðŸ˜¡ I can't believe this @#$%^\"\n",
    "]\n",
    "\n",
    "print(\"Test Text Cleaning:\")\n",
    "print(\"-\" * 60)\n",
    "for text in test_texts:\n",
    "    cleaned = clean_text(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cfb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicare text cleaning pe dataset...\n",
      "âœ… Text cleaning complet!\n",
      "   RÃ¢nduri eliminate (text gol): 0\n",
      "   RÃ¢nduri rÄƒmase: 20000\n"
     ]
    }
   ],
   "source": [
    "print(\"Aplicare text cleaning pe dataset\")\n",
    "\n",
    "df = dataframe.copy()\n",
    "df['text_original'] = df['text']  # PÄƒstrÄƒm originalul pentru referinÈ›Äƒ\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# EliminÄƒ rÃ¢ndurile cu text gol dupÄƒ curÄƒÈ›are\n",
    "empty_before = len(df)\n",
    "df = df[df['text'].str.len() > 0].reset_index(drop=True)\n",
    "empty_after = len(df)\n",
    "\n",
    "print(f\"DONE -- Text cleaning complet!\")\n",
    "print(f\"   RÃ¢nduri eliminate (text gol): {empty_before - empty_after}\")\n",
    "print(f\"   RÃ¢nduri rÄƒmase: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64238b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemple Ã®nainte/dupÄƒ curÄƒÈ›are:\n",
      "======================================================================\n",
      "Original: i was feeling very energetic yesterday i decided to start the a href https www...\n",
      "Cleaned:  i was feeling very energetic yesterday i decided to start the a href www...\n",
      "Label:    2\n",
      "----------------------------------------------------------------------\n",
      "Original: i feel so honoured to receive this from krista know to the blogger world as a href https www...\n",
      "Cleaned:  i feel so honoured to receive this from krista know to the blogger world as a href www...\n",
      "Label:    2\n",
      "----------------------------------------------------------------------\n",
      "Original: i got this very sexy latex outfit from their lucky chair it made me feel very naughty the hair is called hungover and it...\n",
      "Cleaned:  i got this very sexy latex outfit from their lucky chair it made me feel very naughty the hair is called hungover and it...\n",
      "Label:    3\n",
      "----------------------------------------------------------------------\n",
      "Original: im feeling generous lets make it a a href https www...\n",
      "Cleaned:  im feeling generous lets make it a a href www...\n",
      "Label:    3\n",
      "----------------------------------------------------------------------\n",
      "Original: i have the power to make another do what i want but in reality feel threatened and desire to control this other person s...\n",
      "Cleaned:  i have the power to make another do what i want but in reality feel threatened and desire to control this other person s...\n",
      "Label:    1\n",
      "----------------------------------------------------------------------\n",
      "Original: i will be able to let that passion out but at present these little paintings help me feel reassured not to let my dreams...\n",
      "Cleaned:  i will be able to let that passion out but at present these little paintings help me feel reassured not to let my dreams...\n",
      "Label:    2\n",
      "----------------------------------------------------------------------\n",
      "Original: im feeling pretty good now and ignoring the fact that ill probably feel worse before i feel better a href https lh...\n",
      "Cleaned:  im feeling pretty good now and ignoring the fact that ill probably feel worse before i feel better a href lh...\n",
      "Label:    2\n",
      "----------------------------------------------------------------------\n",
      "Original: i feel stumble a class content link href https plusone...\n",
      "Cleaned:  i feel stumble a class content link href plusone...\n",
      "Label:    2\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExemple Ã®nainte/dupÄƒ curÄƒÈ›are:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df['changed'] = df['text'] != df['text_original']\n",
    "changed_examples = df[df['changed']].head(10)\n",
    "\n",
    "for idx, row in changed_examples.iterrows():\n",
    "    print(f\"Original: {row['text_original'][:120]}...\")\n",
    "    print(f\"Cleaned:  {row['text'][:120]}...\")\n",
    "    print(f\"Label:    {row['label']}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "------------------------------\n",
      "  0: anger      (2709 samples)\n",
      "  1: fear       (2373 samples)\n",
      "  2: joy        (6761 samples)\n",
      "  3: love       (1641 samples)\n",
      "  4: sadness    (5797 samples)\n",
      "  5: surprise   (719 samples)\n"
     ]
    }
   ],
   "source": [
    "label_list = sorted(df['label_str'].unique())\n",
    "label_to_id = { label: idx for idx, label in enumerate(label_list) }\n",
    "id_to_label = { idx: label for label, idx in label_to_id.items() }\n",
    "NUMBER_OF_LABELS = len(label_list)\n",
    "\n",
    "print(\"Label Mapping:\")\n",
    "print(\"-\" * 30)\n",
    "for label, idx in label_to_id.items():\n",
    "    count = len(df[df['label_str'] == label])\n",
    "    print(f\"  {idx}: {label:10s} ({count} samples)\")\n",
    "\n",
    "df['label_id'] = df['label_str'].map(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75aaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train: 16,000 (80.0%)\n",
      "   Val:   2,000 (10.0%)\n",
      "   Test:  2,000 (10.0%)\n",
      "   Total: 20,000\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label_id'], random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"   Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Val:   {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Test:  {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Total: {len(train_df) + len(val_df) + len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09543828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Original (%)  Train (%)  Val (%)  Test (%)\n",
      "label                                            \n",
      "2             33.80      33.81    33.80     33.80\n",
      "4             28.98      28.99    29.00     28.95\n",
      "0             13.54      13.54    13.55     13.55\n",
      "1             11.86      11.86    11.85     11.90\n",
      "3              8.20       8.21     8.20      8.20\n",
      "5              3.60       3.59     3.60      3.60\n",
      "\n",
      "âœ… DistribuÈ›iile sunt similare - stratificarea a funcÈ›ionat!\n"
     ]
    }
   ],
   "source": [
    "strat_check = pd.DataFrame({\n",
    "    'Original (%)': (df['label'].value_counts(normalize=True) * 100).round(2),\n",
    "    'Train (%)': (train_df['label'].value_counts(normalize=True) * 100).round(2),\n",
    "    'Val (%)': (val_df['label'].value_counts(normalize=True) * 100).round(2),\n",
    "    'Test (%)': (test_df['label'].value_counts(normalize=True) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(strat_check)\n",
    "print(\"\\nDONE -- DistribuÈ›iile sunt similare - stratificarea a funcÈ›ionat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb1d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    5409\n",
      "4    4638\n",
      "0    2167\n",
      "1    1898\n",
      "3    1313\n",
      "5     575\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total: 16000\n"
     ]
    }
   ],
   "source": [
    "train_dist_before = train_df['label'].value_counts()\n",
    "print(train_dist_before)\n",
    "print(f\"\\nTotal: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Reshape pentru sklearn (necesitÄƒ 2D array)\n",
    "X_train = train_df['text'].values.reshape(-1, 1)\n",
    "y_train = train_df['label_id'].values\n",
    "\n",
    "# Fit È™i resample\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# CreeazÄƒ DataFrame nou cu datele oversampled\n",
    "train_df_oversampled = pd.DataFrame({\n",
    "    'text': X_resampled.flatten(),\n",
    "    'label_id': y_resampled\n",
    "})\n",
    "\n",
    "# AdaugÄƒ Ã®napoi label-ul text\n",
    "train_df_oversampled['label'] = train_df_oversampled['label_id'].map(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "fear        5409\n",
      "joy         5409\n",
      "sadness     5409\n",
      "love        5409\n",
      "anger       5409\n",
      "surprise    5409\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total: 32454\n",
      "\n",
      "ðŸ“ˆ CreÈ™tere: 16000 â†’ 32454 (+16454 samples)\n"
     ]
    }
   ],
   "source": [
    "train_dist_after = train_df_oversampled['label'].value_counts()\n",
    "print(train_dist_after)\n",
    "print(f\"\\nTotal: {len(train_df_oversampled)}\")\n",
    "\n",
    "print(f\"\\n CreÈ™tere: {len(train_df)} â†’ {len(train_df_oversampled)} (+{len(train_df_oversampled) - len(train_df)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6feaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oversampling Stats:\n",
      "{'before': {'total': 16000, 'distribution': {2: 5409, 4: 4638, 0: 2167, 1: 1898, 3: 1313, 5: 575}}, 'after': {'total': 32454, 'distribution': {'fear': 5409, 'joy': 5409, 'sadness': 5409, 'love': 5409, 'anger': 5409, 'surprise': 5409}}, 'increase': 16454, 'increase_pct': 102.8375}\n"
     ]
    }
   ],
   "source": [
    "oversampling_stats = {\n",
    "    'before': {\n",
    "        'total': len(train_df),\n",
    "        'distribution': train_dist_before.to_dict()\n",
    "    },\n",
    "    'after': {\n",
    "        'total': len(train_df_oversampled),\n",
    "        'distribution': train_dist_after.to_dict()\n",
    "    },\n",
    "    'increase': len(train_df_oversampled) - len(train_df),\n",
    "    'increase_pct': (len(train_df_oversampled) - len(train_df)) / len(train_df) * 100\n",
    "}\n",
    "\n",
    "print(\"\\nOversampling Stats:\")\n",
    "print(oversampling_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b79b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Ã®ncÄƒrcat: roberta-base\n",
      "  Vocab size: 50,265\n",
      "  Max length: 128\n",
      "  Padding token: <pad>\n",
      "  Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LENGTH = 128 \n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Tokenizer Ã®ncÄƒrcat: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Padding token: {tokenizer.pad_token}\")\n",
    "print(f\"  Special tokens: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test tokenizare:\n",
      "------------------------------------------------------------\n",
      "Text: I am so happy today!\n",
      "Tokens (6): ['I', 'Ä am', 'Ä so', 'Ä happy', 'Ä today', '!']\n",
      "Input IDs: [0, 100, 524, 98, 1372, 452, 328, 2]...\n",
      "\n",
      "Text: This makes me really angry and frustrated.\n",
      "Tokens (8): ['This', 'Ä makes', 'Ä me', 'Ä really', 'Ä angry', 'Ä and', 'Ä frustrated', '.']\n",
      "Input IDs: [0, 713, 817, 162, 269, 5800, 8, 8164, 4, 2]...\n",
      "\n",
      "Text: I'm kinda scared about what's gonna happen...\n",
      "Tokens (10): ['I', \"'m\", 'Ä kinda', 'Ä scared', 'Ä about', 'Ä what', \"'s\", 'Ä gonna', 'Ä happen', '...']\n",
      "Input IDs: [0, 100, 437, 24282, 8265, 59, 99, 18, 6908, 1369, 734, 2]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest tokenizare:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_texts = [\n",
    "    \"I am so happy today!\",\n",
    "    \"This makes me really angry and frustrated.\",\n",
    "    \"I'm kinda scared about what's gonna happen...\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    encoding = tokenizer(text, truncation=True, max_length=MAX_LENGTH)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"Input IDs: {encoding['input_ids'][:15]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e807ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AnalizÄƒ lungime Ã®n tokeni (pe train oversampled):\n",
      "------------------------------------------------------------\n",
      "  Mean:   19.9 tokens\n",
      "  Median: 18.0 tokens\n",
      "  Max:    67 tokens\n",
      "  Min:    2 tokens\n",
      "  Std:    11.4 tokens\n",
      "\n",
      "  DepÄƒÈ™esc 128 tokens: 0 (0.00%)\n",
      "\n",
      "  Percentila 95: 42 tokens\n",
      "  Percentila 99: 53 tokens\n",
      "\n",
      "âœ… MAX_LENGTH=128 acoperÄƒ 100.0% din texte\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalizÄƒ lungime Ã®n tokeni (pe train oversampled):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "token_lengths = []\n",
    "for text in train_df_oversampled['text'].values[:5000]:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "token_lengths = np.array(token_lengths)\n",
    "\n",
    "print(f\"  Mean:   {token_lengths.mean():.1f} tokens\")\n",
    "print(f\"  Median: {np.median(token_lengths):.1f} tokens\")\n",
    "print(f\"  Max:    {token_lengths.max()} tokens\")\n",
    "print(f\"  Min:    {token_lengths.min()} tokens\")\n",
    "print(f\"  Std:    {token_lengths.std():.1f} tokens\")\n",
    "\n",
    "exceeds = (token_lengths > MAX_LENGTH - 2).sum()\n",
    "print(f\"\\n  DepÄƒÈ™esc {MAX_LENGTH} tokens: {exceeds} ({exceeds/len(token_lengths)*100:.2f}%)\")\n",
    "\n",
    "p95 = np.percentile(token_lengths, 95)\n",
    "p99 = np.percentile(token_lengths, 99)\n",
    "print(f\"\\n  Percentila 95: {p95:.0f} tokens\")\n",
    "print(f\"  Percentila 99: {p99:.0f} tokens\")\n",
    "print(f\"\\nDONE -- MAX_LENGTH={MAX_LENGTH} acoperÄƒ {(token_lengths <= MAX_LENGTH-2).sum()/len(token_lengths)*100:.1f}% din texte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab34d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset pentru emotion classification.\n",
    "    Tokenizarea se face lazy (la __getitem__) pentru eficienÈ›Äƒ.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe['label_id'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenizare - NU facem padding aici (se face Ã®n DataCollator)\n",
    "        encoding = self.tokenizer( text, truncation=True, max_length=self.max_length,\n",
    "            return_tensors=None  # ReturneazÄƒ liste, nu tensori\n",
    "        )\n",
    "        \n",
    "        return { 'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask'], 'labels': label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f05c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Datasets create:\n",
      "   Train (oversampled): 32,454\n",
      "   Val:                 2,000\n",
      "   Test:                2,000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EmotionDataset(train_df_oversampled, tokenizer, MAX_LENGTH)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\DONE -- Datasets create:\")\n",
    "print(f\"   Train (oversampled): {len(train_dataset):,}\")\n",
    "print(f\"   Val:                 {len(val_dataset):,}\")\n",
    "print(f\"   Test:                {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ecd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample din train_dataset:\n",
      "  input_ids (31 tokens): [0, 118, 524, 259, 456, 2157, 10985, 9, 99, 16]...\n",
      "  attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "  label: 1 (fear)\n",
      "\n",
      "  Decoded: <s>i am here again feeling confused of what is happening around me looking for a plane to grasp a reality to settle that feels like it is my own</s>\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(\"\\nSample din train_dataset:\")\n",
    "print(f\"  input_ids ({len(sample['input_ids'])} tokens): {sample['input_ids'][:10]}...\")\n",
    "print(f\"  attention_mask: {sample['attention_mask'][:10]}...\")\n",
    "print(f\"  label: {sample['labels']} ({id_to_label[sample['labels']]})\")\n",
    "print(f\"\\n  Decoded: {tokenizer.decode(sample['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataCollatorWithPadding configurat!\n",
      "   - Padding: dynamic (la max length din batch)\n",
      "   - Return: PyTorch tensors\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding( tokenizer=tokenizer, padding=True, return_tensors='pt' )\n",
    "\n",
    "print(\"DONE --  DataCollatorWithPadding configurat!\")\n",
    "print(\"   - Padding: dynamic (la max length din batch)\")\n",
    "print(\"   - Return: PyTorch tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… DataLoaders create (batch_size=16):\n",
      "   Train: 2029 batches\n",
      "   Val:   125 batches\n",
      "   Test:  125 batches\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16  \n",
    "\n",
    "train_loader = DataLoader( train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=0, pin_memory=True if torch.cuda.is_available() else False )\n",
    "val_loader = DataLoader( val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator, num_workers=0, pin_memory=True if torch.cuda.is_available() else False )\n",
    "test_loader = DataLoader( test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator, num_workers=0, pin_memory=True if torch.cuda.is_available() else False )\n",
    "\n",
    "print(f\"\\nDONE -- DataLoaders create (batch_size={BATCH_SIZE}):\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Val:   {len(val_loader)} batches\")\n",
    "print(f\"   Test:  {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ad67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Dynamic Padding:\n",
      "------------------------------------------------------------\n",
      "Batch 1:\n",
      "  input_ids shape:      torch.Size([16, 48])\n",
      "  attention_mask shape: torch.Size([16, 48])\n",
      "  labels shape:         torch.Size([16])\n",
      "  Max seq length Ã®n batch: 48\n",
      "\n",
      "Batch 2:\n",
      "  input_ids shape:      torch.Size([16, 38])\n",
      "  attention_mask shape: torch.Size([16, 38])\n",
      "  labels shape:         torch.Size([16])\n",
      "  Max seq length Ã®n batch: 38\n",
      "\n",
      "Batch 3:\n",
      "  input_ids shape:      torch.Size([16, 44])\n",
      "  attention_mask shape: torch.Size([16, 44])\n",
      "  labels shape:         torch.Size([16])\n",
      "  Max seq length Ã®n batch: 44\n",
      "\n",
      "âœ… Dynamic padding funcÈ›ioneazÄƒ! (dimensiunea variazÄƒ per batch)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Dynamic Padding:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"  input_ids shape:      {batch['input_ids'].shape}\")\n",
    "    print(f\"  attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"  labels shape:         {batch['labels'].shape}\")\n",
    "    print(f\"  Max seq length Ã®n batch: {batch['input_ids'].shape[1]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Date salvate!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "train_df_oversampled.to_csv('data/processed/train_oversampled.csv', index=False)\n",
    "train_df.to_csv('data/processed/train_original.csv', index=False)\n",
    "val_df.to_csv('data/processed/val.csv', index=False)\n",
    "test_df.to_csv('data/processed/test.csv', index=False)\n",
    "\n",
    "mappings = {\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': {str(k): v for k, v in id_to_label.items()},\n",
    "    'num_labels': NUMBER_OF_LABELS,\n",
    "    'label_list': label_list\n",
    "}\n",
    "\n",
    "with open('data/processed/label_mappings.json', 'w') as f:\n",
    "    json.dump(mappings, f, indent=2)\n",
    "\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'random_state': 42,\n",
    "    'num_labels': NUMBER_OF_LABELS,\n",
    "    'split_sizes': {\n",
    "        'train_original': len(train_df),\n",
    "        'train_oversampled': len(train_df_oversampled),\n",
    "        'val': len(val_df),\n",
    "        'test': len(test_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data/processed/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "with open('reports/oversampling_stats.json', 'w') as f:\n",
    "    json.dump(oversampling_stats, f, indent=2)\n",
    "\n",
    "print(\"âœ… Date salvate!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

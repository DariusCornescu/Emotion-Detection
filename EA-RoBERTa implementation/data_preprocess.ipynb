{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee106a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers datasets torch scikit-learn pandas matplotlib seaborn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bca21489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os \n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, RobertaTokenizerFast\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc88ed",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce5a251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = DATA_PATH = \"../database_used_by_paper\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, 'train.txt')\n",
    "TEST_PATH = os.path.join(DATA_DIR, 'test.txt')\n",
    "VAL_PATH = os.path.join(DATA_DIR, 'val.txt')\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TFIDF_TRASHHOLD = 3.5\n",
    "MIN_TOKENS = 4\n",
    "\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129022c7",
   "metadata": {},
   "source": [
    "## Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b711a2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "   Train set size:, 16,000samples\n",
      "   Validation set size:, 2,000 samples\n",
      "   Test set size:, 2,000 samples\n",
      "\n",
      "Total samples:, 20,000 samples\n"
     ]
    }
   ],
   "source": [
    "def load_emotional_data(file_path, sep=';'):\n",
    "\n",
    "    texts, labels = [], []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "\n",
    "            parts = line.split(sep, 1)  \n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "\n",
    "            texts.append(parts[0].strip())\n",
    "            labels.append(parts[1].strip())\n",
    "\n",
    "    return pd.DataFrame({'text': texts, 'label_str': labels})\n",
    "\n",
    "try:\n",
    "    print(\"Loading data...\")\n",
    "    train_df_raw = load_emotional_data(TRAIN_PATH)\n",
    "    val_df_raw = load_emotional_data(VAL_PATH)\n",
    "    test_df_raw = load_emotional_data(TEST_PATH)\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"   Train set size:, {len(train_df_raw):,}samples\")\n",
    "    print(f\"   Validation set size:, {len(val_df_raw):,} samples\")\n",
    "    print(f\"   Test set size:, {len(test_df_raw):,} samples\")\n",
    "    print(f\"\\nTotal samples:, {len(train_df_raw) + len(val_df_raw) + len(test_df_raw):,} samples\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading data:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217471c1",
   "metadata": {},
   "source": [
    "## Slang-Aware cleaning + text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424c1d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 157 slang mappings\n",
      "Loaded 23 emoticon mappings\n"
     ]
    }
   ],
   "source": [
    "SLANG_MAP = {\n",
    "    # Common abbreviations\n",
    "    \"u\": \"you\", \"ur\": \"your\", \"r\": \"are\", \"n\": \"and\", \"b\": \"be\", \"c\": \"see\", \"y\": \"why\",\n",
    "    \"k\": \"okay\", \"ok\": \"okay\", \"pls\": \"please\", \"plz\": \"please\", \"thx\": \"thanks\", \"thnx\": \"thanks\",\n",
    "    \"ty\": \"thank you\", \"bc\": \"because\", \"cuz\": \"because\", \"bcuz\": \"because\", \"bcoz\": \"because\",\n",
    "    \"cos\": \"because\", \"coz\": \"because\",\"w\": \"with\", \"wo\": \"without\", \"abt\": \"about\", \"bout\": \"about\",\n",
    "    \"bf\": \"boyfriend\", \"gf\": \"girlfriend\", \"bff\": \"best friend\", \"rn\": \"right now\",\n",
    "    \"imo\": \"in my opinion\", \"imho\": \"in my humble opinion\", \"tbh\": \"to be honest\",\n",
    "    \"fyi\": \"for your information\", \"btw\": \"by the way\", \"afaik\": \"as far as i know\",\n",
    "    \"irl\": \"in real life\", \"jk\": \"just kidding\", \"omw\": \"on my way\", \"brb\": \"be right back\",\n",
    "    \"gtg\": \"got to go\", \"g2g\": \"got to go\", \"ttyl\": \"talk to you later\", \"nvm\": \"never mind\",\n",
    "    \"idk\": \"i do not know\", \"idc\": \"i do not care\", \"idgaf\": \"i do not care\", \"dm\": \"direct message\",\n",
    "    \"rt\": \"retweet\", \"fb\": \"facebook\", \"ig\": \"instagram\", \"yt\": \"youtube\", \"smh\": \"shaking my head\",\n",
    "    \"fomo\": \"fear of missing out\", \"yolo\": \"you only live once\", \"bae\": \"babe\", \"fam\": \"family\",\n",
    "    \"lit\": \"exciting\", \"slay\": \"amazing\", \"goat\": \"greatest of all time\", \"af\": \"very\",\n",
    "    \"asf\": \"very\", \"lowkey\": \"somewhat\", \"highkey\": \"very\", \"srsly\": \"seriously\", \"tho\": \"though\",\n",
    "    \"thru\": \"through\", \"kinda\": \"kind of\", \"sorta\": \"sort of\", \"gonna\": \"going to\",\n",
    "    \"wanna\": \"want to\", \"gotta\": \"got to\", \"coulda\": \"could have\", \"shoulda\": \"should have\",\n",
    "    \"woulda\": \"would have\", \"musta\": \"must have\", \"lemme\": \"let me\", \"gimme\": \"give me\",\n",
    "    \"dunno\": \"do not know\", \"whatcha\": \"what are you\", \"gotcha\": \"got you\", \"outta\": \"out of\", \"lotta\": \"lot of\",\n",
    "    \"lotsa\": \"lots of\", \"kinda\": \"kind of\", \"innit\": \"is it not\", \"aint\": \"is not\", \"ain't\": \"is not\",\n",
    "    \"yall\": \"you all\", \"y'all\": \"you all\",\n",
    "    \n",
    "    # Emotion-related expressions \n",
    "    \"lol\": \"laughing\", \"lmao\": \"laughing\", \"lmfao\": \"laughing\", \"rofl\": \"laughing\",\n",
    "    \"roflmao\": \"laughing\", \"haha\": \"laughing\", \"hahaha\": \"laughing\", \"hehe\": \"laughing\",\n",
    "    \"hihi\": \"laughing\", \"xd\": \"laughing\", \"xD\": \"laughing\", \"omg\": \"oh my god\", \"omfg\": \"oh my god\",\n",
    "    \"wtf\": \"what the heck\", \"wth\": \"what the heck\", \"stfu\": \"shut up\", \"ugh\": \"frustrated\",\n",
    "    \"meh\": \"indifferent\", \"oof\": \"ouch\", \"yikes\": \"shocked\", \"eww\": \"disgusted\", \"aww\": \"touched\",\n",
    "    \"yay\": \"excited\", \"woohoo\": \"excited\", \"woah\": \"surprised\", \"whoa\": \"surprised\",\n",
    "    \"wow\": \"surprised\", \"damn\": \"frustrated\", \"dammit\": \"frustrated\", \"damnit\": \"frustrated\",\n",
    "    \n",
    "    # Missing apostrophe contractions\n",
    "    \"im\": \"i am\", \"ive\": \"i have\", \"id\": \"i would\", \"ill\": \"i will\", \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\", \"youd\": \"you would\", \"youll\": \"you will\", \"hes\": \"he is\",\n",
    "    \"shes\": \"she is\", \"thats\": \"that is\", \"whats\": \"what is\", \"whos\": \"who is\",\n",
    "    \"wheres\": \"where is\", \"heres\": \"here is\", \"theres\": \"there is\", \"theyre\": \"they are\",\n",
    "    \"theyve\": \"they have\", \"theyd\": \"they would\", \"theyll\": \"they will\", \"weve\": \"we have\",\n",
    "    \"wed\": \"we would\", \"dont\": \"do not\", \"doesnt\": \"does not\", \"didnt\": \"did not\",\n",
    "    \"wont\": \"will not\", \"wouldnt\": \"would not\", \"couldnt\": \"could not\", \"shouldnt\": \"should not\",\n",
    "    \"cant\": \"cannot\", \"cannot\": \"can not\", \"isnt\": \"is not\", \"arent\": \"are not\",\n",
    "    \"wasnt\": \"was not\", \"werent\": \"were not\", \"hasnt\": \"has not\", \"havent\": \"have not\",\n",
    "    \"hadnt\": \"had not\", \"mustnt\": \"must not\", \"lets\": \"let us\",\n",
    "}\n",
    "\n",
    "# Text emoticons to emotion words \n",
    "EMOTICON_MAP = {\n",
    "    \":)\": \" happy \", \":(\": \" sad \", \":D\": \" very happy \", \":-)\": \" happy \", \":-(\": \" sad \",\n",
    "    \";)\": \" playful \", \";-)\": \" playful \", \":p\": \" playful \", \":P\": \" playful \", \":-p\": \" playful \",\n",
    "    \":-P\": \" playful \", \":o\": \" surprised \", \":O\": \" surprised \", \":-o\": \" surprised \",\n",
    "    \":-O\": \" surprised \", \":/\": \" unsure \", \":-/\": \" unsure \", \":\\\\\": \" unsure \", \":-\\\\\": \" unsure \",\n",
    "    \"<3\": \" love \", \"</3\": \" heartbroken \", \"xo\": \" love \", \"xoxo\": \" love \",\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(SLANG_MAP)} slang mappings\")\n",
    "print(f\"Loaded {len(EMOTICON_MAP)} emoticon mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33b9243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_elongated_words(text):\n",
    "    \"\"\"\n",
    "    Reduce elongated characters to max 2 repetitions.\n",
    "    'soooooo' -> 'soo', 'happyyy' -> 'happyy'\n",
    "    This preserves some emphasis while normalizing.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "def replace_slang(text):\n",
    "    \"\"\"\n",
    "    Replace slang/abbreviations with full forms.\n",
    "    Uses word boundaries to avoid partial replacements.\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in SLANG_MAP:\n",
    "            result.append(SLANG_MAP[word_lower])\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    \"\"\"Replace text emoticons with emotion words.\"\"\"\n",
    "\n",
    "    for emoticon, replacement in EMOTICON_MAP.items():\n",
    "        text = text.replace(emoticon, replacement)\n",
    "    return text\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    \"\"\"Replace text emoticons with emotion words.\"\"\"\n",
    "\n",
    "    for emoticon, replacement in EMOTICON_MAP.items():\n",
    "        text = text.replace(emoticon, replacement)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Comprehensive text cleaning for social media data.\"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Replace emoticons with emotion words (before stripping special chars)\n",
    "    text = replace_emoticons(text)\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 4. Convert hashtags to words (#happy -> happy)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # 5. Normalize elongated words (soooo -> soo)\n",
    "    text = normalize_elongated_words(text)\n",
    "    \n",
    "    # 6. Replace slang and abbreviations\n",
    "    text = replace_slang(text)\n",
    "    \n",
    "    # 7. Remove special characters but keep emotional punctuation\n",
    "    # Keep: letters, numbers, spaces, and ! ? . , ' - \"\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s!?.,;:'\\\"\\-]\", '', text)\n",
    "    \n",
    "    # 8. Normalize repeated punctuation (!!!! -> !!)\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # 9. Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 10. Strip and lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf48568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SLANG-AWARE TEXT CLEANING EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "Original: I'm SO happy!!! ðŸ˜Š https://example.com @friend #blessed\n",
      "Cleaned:  i'm so happy!! unsure example.com blessed\n",
      "\n",
      "Original: omg u r soooo funny lol :D\n",
      "Cleaned:  oh my god you are soo funny laughing very happy\n",
      "\n",
      "Original: idk why im feeling sad rn tbh :(\n",
      "Cleaned:  i do not know why i am feeling sad right now to be honest sad\n",
      "\n",
      "Original: cant believe this happened smh\n",
      "Cleaned:  cannot believe this happened shaking my head\n",
      "\n",
      "Original: gonna miss u bf </3 ttyl\n",
      "Cleaned:  going to miss you boyfriend heartbroken talk to you later\n",
      "\n",
      "Original: thx for the help! ur the best <3\n",
      "Cleaned:  thanks for the help! your the best love\n",
      "\n",
      "Original: ANGRY!!! ðŸ˜¡ I can't believe this wtf\n",
      "Cleaned:  angry!! i can't believe this what the heck\n",
      "\n",
      "Original: im lowkey scared ngl\n",
      "Cleaned:  i am somewhat scared ngl\n",
      "\n",
      "Original: yall shouldnt have done that\n",
      "Cleaned:  you all should not have done that\n",
      "\n",
      "Original: Normal text with some punctuation, like this.\n",
      "Cleaned:  normal text with some punctuation, like this.\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"I'm SO happy!!! ðŸ˜Š https://example.com @friend #blessed\",\n",
    "    \"omg u r soooo funny lol :D\",\n",
    "    \"idk why im feeling sad rn tbh :(\",\n",
    "    \"cant believe this happened smh\",\n",
    "    \"gonna miss u bf </3 ttyl\",\n",
    "    \"thx for the help! ur the best <3\",\n",
    "    \"ANGRY!!! ðŸ˜¡ I can't believe this wtf\",\n",
    "    \"im lowkey scared ngl\",\n",
    "    \"yall shouldnt have done that\",\n",
    "    \"Normal text with some punctuation, like this.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SLANG-AWARE TEXT CLEANING EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "for text in test_texts:\n",
    "    cleaned = clean_text(text)\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c3add",
   "metadata": {},
   "source": [
    "## Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433ecafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping (built from TRAIN only):\n",
      "----------------------------------------\n",
      "  0: anger      (2,159 train samples)\n",
      "  1: fear       (1,937 train samples)\n",
      "  2: joy        (5,362 train samples)\n",
      "  3: love       (1,304 train samples)\n",
      "  4: sadness    (4,666 train samples)\n",
      "  5: surprise   (572 train samples)\n",
      "\n",
      "âœ… All val/test labels exist in train - mapping is consistent!\n"
     ]
    }
   ],
   "source": [
    "train_labels = sorted(train_df_raw['label_str'].unique())\n",
    "\n",
    "label_to_id = {label: idx for idx, label in enumerate(train_labels)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "NUM_LABELS = len(train_labels)\n",
    "\n",
    "print(\"Label Mapping (built from TRAIN only):\")\n",
    "print(\"-\" * 40)\n",
    "for label, idx in label_to_id.items():\n",
    "    count = len(train_df_raw[train_df_raw['label_str'] == label])\n",
    "    print(f\"  {idx}: {label:10s} ({count:,} train samples)\")\n",
    "\n",
    "val_labels = set(val_df_raw['label_str'].unique())\n",
    "test_labels = set(test_df_raw['label_str'].unique())\n",
    "train_label_set = set(train_labels)\n",
    "\n",
    "unknown_val = val_labels - train_label_set\n",
    "unknown_test = test_labels - train_label_set\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… All val/test labels exist in train - mapping is consistent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f84ca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Consistent label mapping applied to all splits!\n",
      "\n",
      "Sample from each split:\n",
      "Train[0]: 'sadness' -> 4\n",
      "Val[0]:   'sadness' -> 4\n",
      "Test[0]:  'sadness' -> 4\n"
     ]
    }
   ],
   "source": [
    "def apply_label_mapping(df, label_to_id):\n",
    "\n",
    "    df = df.copy()    \n",
    "    df['label'] = df['label_str'].map(label_to_id)\n",
    "    return df\n",
    "\n",
    "train_df_raw = apply_label_mapping(train_df_raw, label_to_id)\n",
    "val_df_raw = apply_label_mapping(val_df_raw, label_to_id)\n",
    "test_df_raw = apply_label_mapping(test_df_raw, label_to_id)\n",
    "\n",
    "print(\"âœ… Consistent label mapping applied to all splits!\")\n",
    "\n",
    "print(\"\\nSample from each split:\")\n",
    "print(f\"Train[0]: '{train_df_raw.iloc[0]['label_str']}' -> {train_df_raw.iloc[0]['label']}\")\n",
    "print(f\"Val[0]:   '{val_df_raw.iloc[0]['label_str']}' -> {val_df_raw.iloc[0]['label']}\")\n",
    "print(f\"Test[0]:  '{test_df_raw.iloc[0]['label_str']}' -> {test_df_raw.iloc[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e260b",
   "metadata": {},
   "source": [
    "## Apply Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee1b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying slang-aware text cleaning...\n",
      "\n",
      "âœ… Text cleaning complete!\n",
      "   Train: 16,000 samples (removed 0)\n",
      "   Val:   2,000 samples (removed 0)\n",
      "   Test:  2,000 samples (removed 0)\n"
     ]
    }
   ],
   "source": [
    "def apply_cleaning(df):\n",
    "    \"\"\"Apply text cleaning and remove empty results.\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['text_original'] = df['text']\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    before = len(df)\n",
    "    df = df[df['text'].str.len() > 0].reset_index(drop=True)\n",
    "    after = len(df)\n",
    "    \n",
    "    return df, before - after\n",
    "\n",
    "print(\"Applying slang-aware text cleaning...\")\n",
    "\n",
    "train_df, train_removed = apply_cleaning(train_df_raw)\n",
    "val_df, val_removed = apply_cleaning(val_df_raw)\n",
    "test_df, test_removed = apply_cleaning(test_df_raw)\n",
    "\n",
    "print(f\"\\nâœ… Text cleaning complete!\")\n",
    "print(f\"   Train: {len(train_df):,} samples (removed {train_removed})\")\n",
    "print(f\"   Val:   {len(val_df):,} samples (removed {val_removed})\")\n",
    "print(f\"   Test:  {len(test_df):,} samples (removed {test_removed})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8489663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of cleaned text:\n",
      "======================================================================\n",
      "\n",
      "Original: i didnt feel humiliated...\n",
      "Cleaned:  i did not feel humiliated...\n",
      "Label:    sadness\n",
      "\n",
      "Original: im grabbing a minute to post i feel greedy wrong...\n",
      "Cleaned:  i am grabbing a minute to post i feel greedy wrong...\n",
      "Label:    anger\n",
      "\n",
      "Original: ive been feeling a little burdened lately wasnt sure why that was...\n",
      "Cleaned:  i have been feeling a little burdened lately was not sure why that was...\n",
      "Label:    sadness\n",
      "\n",
      "Original: ive been taking or milligrams or times recommended amount and ive fallen asleep ...\n",
      "Cleaned:  i have been taking or milligrams or times recommended amount and i have fallen a...\n",
      "Label:    surprise\n",
      "\n",
      "Original: i didnt really feel that embarrassed...\n",
      "Cleaned:  i did not really feel that embarrassed...\n",
      "Label:    sadness\n",
      "\n",
      "Original: i already feel like i fucked up though because i dont usually eat at all in the ...\n",
      "Cleaned:  i already feel like i fucked up though because i do not usually eat at all in th...\n",
      "Label:    anger\n",
      "\n",
      "Original: i feel so inhibited in someone elses kitchen like im painting on someone elses p...\n",
      "Cleaned:  i feel so inhibited in someone elses kitchen like i am painting on someone elses...\n",
      "Label:    sadness\n",
      "\n",
      "Original: i feel kinda appalled that she feels like she needs to explain in wide and lengh...\n",
      "Cleaned:  i feel kind of appalled that she feels like she needs to explain in wide and len...\n",
      "Label:    anger\n",
      "\n",
      "Original: i climbed the hill feeling frustrated that id pretty much paced entirely wrong f...\n",
      "Cleaned:  i climbed the hill feeling frustrated that i would pretty much paced entirely wr...\n",
      "Label:    anger\n",
      "\n",
      "Original: ive worn it once on its own with a little concealer and for the days im feeling ...\n",
      "Cleaned:  i have worn it once on its own with a little concealer and for the days i am fee...\n",
      "Label:    joy\n"
     ]
    }
   ],
   "source": [
    "train_df['changed'] = train_df['text'] != train_df['text_original'].apply(lambda x: x.lower())\n",
    "changed = train_df[train_df['changed']].head(10)\n",
    "\n",
    "print(\"Examples of cleaned text:\")\n",
    "print(\"=\" * 70)\n",
    "for idx, row in changed.iterrows():\n",
    "    print(f\"\\nOriginal: {row['text_original'][:80]}...\")\n",
    "    print(f\"Cleaned:  {row['text'][:80]}...\")\n",
    "    print(f\"Label:    {row['label_str']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d9654",
   "metadata": {},
   "source": [
    "## Check Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df1f40f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Across Splits:\n",
      "============================================================\n",
      "           Train  Train %  Val  Val %  Test  Test %\n",
      "label_str                                          \n",
      "joy         5362    33.51  704  35.20   695   34.75\n",
      "sadness     4666    29.16  550  27.50   581   29.05\n",
      "anger       2159    13.49  275  13.75   275   13.75\n",
      "fear        1937    12.11  212  10.60   224   11.20\n",
      "love        1304     8.15  178   8.90   159    7.95\n",
      "surprise     572     3.58   81   4.05    66    3.30\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Distribution Across Splits:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dist_df = pd.DataFrame({\n",
    "    'Train': train_df['label_str'].value_counts(),\n",
    "    'Val': val_df['label_str'].value_counts(),\n",
    "    'Test': test_df['label_str'].value_counts()\n",
    "})\n",
    "\n",
    "dist_df['Train %'] = (dist_df['Train'] / len(train_df) * 100).round(2)\n",
    "dist_df['Val %'] = (dist_df['Val'] / len(val_df) * 100).round(2)\n",
    "dist_df['Test %'] = (dist_df['Test'] / len(test_df) * 100).round(2)\n",
    "\n",
    "print(dist_df[['Train', 'Train %', 'Val', 'Val %', 'Test', 'Test %']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074a809",
   "metadata": {},
   "source": [
    "## Oversampling (Train Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c44d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution BEFORE oversampling:\n",
      "label_str\n",
      "joy         5362\n",
      "sadness     4666\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train distribution AFTER oversampling:\n",
      "label_str\n",
      "sadness     5362\n",
      "anger       5362\n",
      "love        5362\n",
      "surprise    5362\n",
      "fear        5362\n",
      "joy         5362\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… Oversampling complete!\n",
      "   Before: 16,000\n",
      "   After:  32,172\n",
      "   Added:  16,172 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Train distribution BEFORE oversampling:\")\n",
    "print(train_df['label_str'].value_counts())\n",
    "\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "\n",
    "X_train = train_df['text'].values.reshape(-1, 1)\n",
    "y_train = train_df['label'].values\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create oversampled DataFrame\n",
    "train_df_oversampled = pd.DataFrame({ 'text': X_resampled.flatten(), 'label': y_resampled })\n",
    "train_df_oversampled['label_str'] = train_df_oversampled['label'].map(id_to_label)\n",
    "\n",
    "print(f\"\\nTrain distribution AFTER oversampling:\")\n",
    "print(train_df_oversampled['label_str'].value_counts())\n",
    "\n",
    "print(f\"\\nâœ… Oversampling complete!\")\n",
    "print(f\"   Before: {len(train_df):,}\")\n",
    "print(f\"   After:  {len(train_df_oversampled):,}\")\n",
    "print(f\"   Added:  {len(train_df_oversampled) - len(train_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a40c0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling stats saved to reports/oversampling_stats.json\n",
      "{'before': {'total': 16000, 'distribution': {'joy': 5362, 'sadness': 4666, 'anger': 2159, 'fear': 1937, 'love': 1304, 'surprise': 572}}, 'after': {'total': 32172, 'distribution': {'sadness': 5362, 'anger': 5362, 'love': 5362, 'surprise': 5362, 'fear': 5362, 'joy': 5362}}, 'increase': 16172, 'increase_pct': 101.08}\n"
     ]
    }
   ],
   "source": [
    "# Save oversampling statistics\n",
    "oversampling_stats = {\n",
    "    'before': {\n",
    "        'total': len(train_df),\n",
    "        'distribution': train_df['label_str'].value_counts().to_dict()\n",
    "    },\n",
    "    'after': {\n",
    "        'total': len(train_df_oversampled),\n",
    "        'distribution': train_df_oversampled['label_str'].value_counts().to_dict()\n",
    "    },\n",
    "    'increase': len(train_df_oversampled) - len(train_df),\n",
    "    'increase_pct': round((len(train_df_oversampled) - len(train_df)) / len(train_df) * 100, 2)\n",
    "}\n",
    "\n",
    "with open('reports/oversampling_stats.json', 'w') as f:\n",
    "    json.dump(oversampling_stats, f, indent=2)\n",
    "    \n",
    "print(\"Oversampling stats saved to reports/oversampling_stats.json\")\n",
    "print(oversampling_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce9d8f",
   "metadata": {},
   "source": [
    "## 6. TF-IDF Gating Mechanism\n",
    "\n",
    "From the paper:\n",
    "> \"The TF-IDF based gating mechanism is introduced to refine attention distribution... computed after oversampling, filtering out words with low significance using a threshold of 3.5... minimum of four tokens per sample is preserved.\"\n",
    "\n",
    "The paper uses TF-IDF for **two purposes**:\n",
    "1. **Hard filtering** (preprocessing): removes low TF-IDF words before tokenization\n",
    "2. **Soft gating** (model): modulates token embeddings based on TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFGating:\n",
    "    def __init__(self, threshold=3.5, min_tokens=4, min_df=2, max_df=0.95):\n",
    "        self.threshold = threshold\n",
    "        self.min_tokens = min_tokens\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            min_df=min_df,           # we are ignoring words appearing in < 2 documents\n",
    "            max_df=max_df,           # we are ignoring as well  words appearing in > 95% of documents \n",
    "            token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "            lowercase=True,\n",
    "            norm = None\n",
    "        )\n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Fit on oversampled training corpus.\"\"\"\n",
    "\n",
    "        print(f\"Fitting TF-IDF on {len(texts):,} samples...\")\n",
    "        self.vectorizer.fit(texts)\n",
    "        self._is_fitted = True\n",
    "        print(f\"  Vocabulary size: {len(self.vectorizer.vocabulary_):,}\")\n",
    "        return self\n",
    "    \n",
    "    def get_word_scores(self, text):\n",
    "        \"\"\"Get TF-IDF scores for each word.\"\"\"\n",
    "\n",
    "        analyzer = self.vectorizer.build_analyzer()\n",
    "        words = analyzer(text.lower())\n",
    "        if not words:\n",
    "            return [], np.array([])\n",
    "        \n",
    "        tfidf = self.vectorizer.transform([text.lower()])\n",
    "        vocab = self.vectorizer.vocabulary_\n",
    "        \n",
    "        scores = []\n",
    "        for word in words:\n",
    "            idx = vocab.get(word)\n",
    "            scores.append(float(tfidf[0, idx]) if idx is not None else 0.0)\n",
    "        \n",
    "        return words, np.array(scores, dtype=np.float32)\n",
    "    \n",
    "    def get_keep_mask(self, scores):\n",
    "        \"\"\"Apply threshold with minimum tokens guarantee.\"\"\"\n",
    "\n",
    "        if len(scores) == 0:\n",
    "            return np.array([], dtype=bool)\n",
    "        \n",
    "        keep = scores >= self.threshold\n",
    "        \n",
    "        # Ensure minimum tokens\n",
    "        if keep.sum() < self.min_tokens:\n",
    "            if len(scores) >= self.min_tokens:\n",
    "                top_k = np.argsort(-scores)[:self.min_tokens]\n",
    "                keep = np.zeros_like(keep, dtype=bool)\n",
    "                keep[top_k] = True\n",
    "            else:\n",
    "                keep = np.ones_like(keep, dtype=bool)\n",
    "        \n",
    "        return keep\n",
    "    \n",
    "    def filter_text(self, text):\n",
    "        \"\"\"Hard filtering: remove low TF-IDF words.\"\"\"\n",
    "\n",
    "        words, scores = self.get_word_scores(text)\n",
    "        if len(words) == 0:\n",
    "            return text\n",
    "        \n",
    "        keep = self.get_keep_mask(scores)\n",
    "        filtered = [w for w, k in zip(words, keep) if k]\n",
    "        return \" \".join(filtered) if filtered else text\n",
    "\n",
    "    def compute_gates(self, text, normalize=True):\n",
    "        words, scores = self.get_word_scores(text)\n",
    "        if len(words) == 0:\n",
    "            return words, np.array([])\n",
    "\n",
    "        keep = self.get_keep_mask(scores)\n",
    "\n",
    "        gates = scores.copy()\n",
    "        gates[~keep] = 0.0\n",
    "\n",
    "        # Ensure certain words always have strong gates\n",
    "        NEGATION_WHITELIST = {\"not\", \"no\", \"never\", \"n't\", \"none\", \"nothing\", \"nowhere\"}\n",
    "        INTENSIFIERS = {\"very\", \"so\", \"too\", \"really\", \"extremely\", \"quite\"}\n",
    "\n",
    "        for i, w in enumerate(words):\n",
    "            if w in NEGATION_WHITELIST or w in INTENSIFIERS:\n",
    "                gates[i] = max(gates[i], 1.0)  # force strong signal\n",
    "\n",
    "        if normalize and gates.max() > 0:\n",
    "            gates = gates / gates.max()\n",
    "\n",
    "        return words, gates.astype(np.float32)\n",
    "    \n",
    "    def get_stats(self, texts):\n",
    "        \"\"\"Get filtering statistics.\"\"\"\n",
    "        \n",
    "        orig_lens, filt_lens = [], []\n",
    "        for text in texts:\n",
    "            words, scores = self.get_word_scores(text)\n",
    "            keep = self.get_keep_mask(scores)\n",
    "            orig_lens.append(len(words))\n",
    "            filt_lens.append(keep.sum())\n",
    "        \n",
    "        return {\n",
    "            'avg_original': np.mean(orig_lens),\n",
    "            'avg_filtered': np.mean(filt_lens),\n",
    "            'avg_retention': np.mean([f/o if o > 0 else 1 for o, f in zip(orig_lens, filt_lens)])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba4a3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF on 32,172 samples...\n",
      "  Vocabulary size: 10,081\n",
      "\n",
      "TF-IDF Configuration:\n",
      "  Threshold: 3.5\n",
      "  Min tokens: 4\n"
     ]
    }
   ],
   "source": [
    "tfidf_gating = TFIDFGating( threshold=TFIDF_TRASHHOLD, min_tokens=MIN_TOKENS )\n",
    "tfidf_gating.fit(train_df_oversampled['text'].tolist())\n",
    "\n",
    "print(f\"\\nTF-IDF Configuration:\")\n",
    "print(f\"  Threshold: {TFIDF_TRASHHOLD}\")\n",
    "print(f\"  Min tokens: {MIN_TOKENS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ace344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering Statistics:\n",
      "========================================\n",
      "Train (OS):\n",
      "  Avg words before: 19.6\n",
      "  Avg words after:  12.8\n",
      "  Retention rate:   62.8%\n",
      "Val:\n",
      "  Avg words before: 19.2\n",
      "  Avg words after:  11.9\n",
      "  Retention rate:   60.2%\n",
      "Test:\n",
      "  Avg words before: 19.5\n",
      "  Avg words after:  12.1\n",
      "  Retention rate:   60.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFiltering Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for name, df in [('Train (OS)', train_df_oversampled), ('Val', val_df), ('Test', test_df)]:\n",
    "    stats = tfidf_gating.get_stats(df['text'].tolist())\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Avg words before: {stats['avg_original']:.1f}\")\n",
    "    print(f\"  Avg words after:  {stats['avg_filtered']:.1f}\")\n",
    "    print(f\"  Retention rate:   {stats['avg_retention']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30bccec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering Examples:\n",
      "======================================================================\n",
      "\n",
      "Original (5 words):\n",
      "  i did not feel humiliated\n",
      "Filtered (4 words):\n",
      "  did not feel humiliated\n",
      "Gates: i:0.00 did:0.60 not:0.37 feel:0.20 humiliated:1.00 \n",
      "\n",
      "Original (21 words):\n",
      "  i can go from feeling so hopeless to so damned hopeful just from being around so...\n",
      "Filtered (16 words):\n",
      "  can go from so hopeless so damned hopeful just from being around someone who car...\n",
      "Gates: i:0.00 can:0.44 go:0.54 from:0.96 feeling:0.00 so:0.67 hopeless:0.79 to:0.00 ...\n",
      "\n",
      "Original (11 words):\n",
      "  i am grabbing a minute to post i feel greedy wrong\n",
      "Filtered (5 words):\n",
      "  grabbing minute post greedy wrong\n",
      "Gates: i:0.00 am:0.00 grabbing:1.00 a:0.00 minute:0.88 to:0.00 post:0.69 i:0.00 ...\n",
      "\n",
      "Original (18 words):\n",
      "  i am ever feeling nostalgic about the fireplace i will know that it is still on ...\n",
      "Filtered (9 words):\n",
      "  ever nostalgic the fireplace will know still the property\n",
      "Gates: i:0.00 am:0.00 ever:0.57 feeling:0.00 nostalgic:0.65 about:0.00 the:0.43 fireplace:1.00 ...\n",
      "\n",
      "Original (4 words):\n",
      "  i am feeling grouchy\n",
      "Filtered (4 words):\n",
      "  i am feeling grouchy\n",
      "Gates: i:0.00 am:0.34 feeling:0.30 grouchy:1.00 \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFiltering Examples:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sample_texts = train_df_oversampled['text'].head(5).tolist()\n",
    "\n",
    "for text in sample_texts:\n",
    "    filtered = tfidf_gating.filter_text(text)\n",
    "    words, gates = tfidf_gating.compute_gates(text)\n",
    "    \n",
    "    print(f\"\\nOriginal ({len(text.split())} words):\")\n",
    "    print(f\"  {text[:80]}...\" if len(text) > 80 else f\"  {text}\")\n",
    "    print(f\"Filtered ({len(filtered.split())} words):\")\n",
    "    print(f\"  {filtered[:80]}...\" if len(filtered) > 80 else f\"  {filtered}\")\n",
    "    \n",
    "    print(f\"Gates: \", end=\"\")\n",
    "    for w, g in list(zip(words, gates))[:8]:\n",
    "        print(f\"{w}:{g:.2f}\", end=\" \")\n",
    "    if len(words) > 8:\n",
    "        print(\"...\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "98d137b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping hard filtering (will use soft gating only)\n"
     ]
    }
   ],
   "source": [
    "USE_HARD_FILTERING = False  # Set to False to skip hard filtering\n",
    "\n",
    "if USE_HARD_FILTERING:\n",
    "    print(\"Applying hard filtering...\")\n",
    "    \n",
    "    train_df_oversampled['text_filtered'] = train_df_oversampled['text'].apply(tfidf_gating.filter_text)\n",
    "    val_df['text_filtered'] = val_df['text'].apply(tfidf_gating.filter_text)\n",
    "    test_df['text_filtered'] = test_df['text'].apply(tfidf_gating.filter_text)\n",
    "    \n",
    "    orig_len = train_df_oversampled['text'].apply(lambda x: len(x.split())).mean()\n",
    "    filt_len = train_df_oversampled['text_filtered'].apply(lambda x: len(x.split())).mean()\n",
    "    \n",
    "    print(f\"\\nâœ… Hard filtering applied!\")\n",
    "    print(f\"   Avg words: {orig_len:.1f} -> {filt_len:.1f} ({filt_len/orig_len*100:.1f}% retained)\")\n",
    "else:\n",
    "    print(\"Skipping hard filtering (will use soft gating only)\")\n",
    "    train_df_oversampled['text_filtered'] = train_df_oversampled['text']\n",
    "    val_df['text_filtered'] = val_df['text']\n",
    "    test_df['text_filtered'] = test_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aedc4c8",
   "metadata": {},
   "source": [
    "### Dataset with Soft Gating Support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c5c692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: roberta-base\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDatasetWithGates(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, tfidf_gating, max_length=128, compute_gates=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tfidf_gating = tfidf_gating\n",
    "        self.max_length = max_length\n",
    "        self.compute_gates = compute_gates\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize with offset mapping for gate alignment\n",
    "        encoding = self.tokenizer( text, truncation=True, max_length=self.max_length, padding=False, return_offsets_mapping=True, return_tensors=None )\n",
    "\n",
    "        item = { 'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask'], 'labels': label }\n",
    "        if self.compute_gates:\n",
    "            # Compute word-level gates\n",
    "            words, word_gates = self.tfidf_gating.compute_gates(text, normalize=True)\n",
    "            \n",
    "            # Map word gates to token gates\n",
    "            offsets = encoding['offset_mapping']\n",
    "            token_gates = self._map_word_gates_to_tokens(text, words, word_gates, offsets)\n",
    "            item['gates'] = token_gates\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def _map_word_gates_to_tokens(self, text, words, word_gates, offsets):\n",
    "        \"\"\"Map word-level gates to subtoken-level gates.\"\"\"\n",
    "\n",
    "        seq_len = len(offsets)\n",
    "        token_gates = [0.0] * seq_len  \n",
    "        \n",
    "        if len(words) == 0:\n",
    "            return token_gates\n",
    "        \n",
    "        # Build word spans\n",
    "        word_spans = []\n",
    "        pos = 0\n",
    "        text_lower = text.lower()\n",
    "        for word in words:\n",
    "            start = text_lower.find(word, pos)\n",
    "            if start == -1:\n",
    "                word_spans.append(None)\n",
    "                continue\n",
    "            end = start + len(word)\n",
    "            word_spans.append((start, end))\n",
    "            pos = end\n",
    "        \n",
    "        # Map tokens to words\n",
    "        for token_idx, (char_start, char_end) in enumerate(offsets):\n",
    "            if char_start == char_end:  \n",
    "                token_gates[token_idx] = 1.0\n",
    "                continue\n",
    "            \n",
    "            # Find overlapping word\n",
    "            for word_idx, (ws, we) in enumerate(word_spans):\n",
    "                if not (char_end <= ws or char_start >= we):  # Overlap\n",
    "                    token_gates[token_idx] = float(word_gates[word_idx])\n",
    "                    break\n",
    "        \n",
    "        return token_gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b2841f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorWithGates:\n",
    "    \"\"\"Collator that handles padding for both tokens and gates.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, include_gates=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.include_gates = include_gates\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        max_len = max(len(f['input_ids']) for f in features)\n",
    "        \n",
    "        batch = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "        if self.include_gates and 'gates' in features[0]:\n",
    "            batch['gates'] = []\n",
    "        \n",
    "        for f in features:\n",
    "            pad_len = max_len - len(f['input_ids'])\n",
    "            \n",
    "            batch['input_ids'].append(f['input_ids'] + [self.pad_id] * pad_len)\n",
    "            batch['attention_mask'].append(f['attention_mask'] + [0] * pad_len)\n",
    "            batch['labels'].append(f['labels'])\n",
    "            \n",
    "            if 'gates' in batch:\n",
    "                batch['gates'].append(f['gates'] + [0.0] * pad_len)\n",
    "        \n",
    "        batch['input_ids'] = torch.tensor(batch['input_ids'], dtype=torch.long)\n",
    "        batch['attention_mask'] = torch.tensor(batch['attention_mask'], dtype=torch.long)\n",
    "        batch['labels'] = torch.tensor(batch['labels'], dtype=torch.long)\n",
    "        \n",
    "        if 'gates' in batch:\n",
    "            batch['gates'] = torch.tensor(batch['gates'], dtype=torch.float32)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "986dc769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datasets created:\n",
      "   Train: 32,172\n",
      "   Val:   2,000\n",
      "   Test:  2,000\n",
      "\n",
      "   Using: text\n",
      "   Soft gating: True\n"
     ]
    }
   ],
   "source": [
    "# Choose which text to use\n",
    "USE_FILTERED_TEXT = False  # True = use hard-filtered text, False = use original\n",
    "USE_SOFT_GATING = True    # True = compute gates for embedding modulation\n",
    "\n",
    "text_col = 'text_filtered' if USE_FILTERED_TEXT else 'text'\n",
    "\n",
    "train_dataset = EmotionDatasetWithGates(\n",
    "    texts=train_df_oversampled[text_col].tolist(),\n",
    "    labels=train_df_oversampled['label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    tfidf_gating=tfidf_gating,\n",
    "    max_length=MAX_LENGTH,\n",
    "    compute_gates=USE_SOFT_GATING\n",
    ")\n",
    "\n",
    "val_dataset = EmotionDatasetWithGates(\n",
    "    texts=val_df[text_col].tolist(),\n",
    "    labels=val_df['label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    tfidf_gating=tfidf_gating,\n",
    "    max_length=MAX_LENGTH,\n",
    "    compute_gates=USE_SOFT_GATING\n",
    ")\n",
    "\n",
    "test_dataset = EmotionDatasetWithGates(\n",
    "    texts=test_df[text_col].tolist(),\n",
    "    labels=test_df['label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    tfidf_gating=tfidf_gating,\n",
    "    max_length=MAX_LENGTH,\n",
    "    compute_gates=USE_SOFT_GATING\n",
    ")\n",
    "\n",
    "print(f\"âœ… Datasets created:\")\n",
    "print(f\"   Train: {len(train_dataset):,}\")\n",
    "print(f\"   Val:   {len(val_dataset):,}\")\n",
    "print(f\"   Test:  {len(test_dataset):,}\")\n",
    "print(f\"\\n   Using: {text_col}\")\n",
    "print(f\"   Soft gating: {USE_SOFT_GATING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b0ea5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataLoaders created (batch_size=16)\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorWithGates(tokenizer, include_gates=USE_SOFT_GATING)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "\n",
    "print(f\"âœ… DataLoaders created (batch_size={BATCH_SIZE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "444b4b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch:\n",
      "  input_ids:      torch.Size([16, 52])\n",
      "  attention_mask: torch.Size([16, 52])\n",
      "  labels:         torch.Size([16])\n",
      "  gates:          torch.Size([16, 52])\n",
      "  gates range:    [0.000, 1.000]\n",
      "\n",
      "Token-Gate alignment (sample 0):\n",
      "  <s>             -> 1.000\n",
      "  i               -> 0.000\n",
      "  Ä feel           -> 0.000\n",
      "  Ä terrible       -> 1.000\n",
      "  Ä for            -> 0.000\n",
      "  Ä him            -> 0.655\n",
      "  Ä but            -> 0.000\n",
      "  Ä oh             -> 0.998\n",
      "  Ä my             -> 0.000\n",
      "  Ä god            -> 0.839\n",
      "  </s>            -> 1.000\n",
      "  <pad>           -> 0.000\n",
      "  <pad>           -> 0.000\n",
      "  <pad>           -> 0.000\n",
      "  <pad>           -> 0.000\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Sample batch:\")\n",
    "print(f\"  input_ids:      {batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  labels:         {batch['labels'].shape}\")\n",
    "\n",
    "if 'gates' in batch:\n",
    "    print(f\"  gates:          {batch['gates'].shape}\")\n",
    "    print(f\"  gates range:    [{batch['gates'].min():.3f}, {batch['gates'].max():.3f}]\")\n",
    "    \n",
    "    print(\"\\nToken-Gate alignment (sample 0):\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][0].tolist())\n",
    "    gates = batch['gates'][0].tolist()\n",
    "    for t, g in list(zip(tokens, gates))[:15]:\n",
    "        print(f\"  {t:15s} -> {g:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba5170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All data saved!\n",
      "\n",
      "Files:\n",
      "  - data/processed/gate/train_oversampled.csv\n",
      "  - data/processed/gate/tfidf_gating.joblib\n",
      "  - data/processed/gate/label_mappings.json\n",
      "  - data/processed/gate/config.json\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "train_df_oversampled.to_csv('data/processed/gate/train_oversampled.csv', index=False)\n",
    "train_df.to_csv('data/processed/gate/train_original.csv', index=False)\n",
    "val_df.to_csv('data/processed/gate/val.csv', index=False)\n",
    "test_df.to_csv('data/processed/gate/test.csv', index=False)\n",
    "\n",
    "tfidf_state = {\n",
    "    'vectorizer': tfidf_gating.vectorizer,\n",
    "    'threshold': tfidf_gating.threshold,\n",
    "    'min_tokens': tfidf_gating.min_tokens\n",
    "}\n",
    "joblib.dump(tfidf_state, 'data/processed/gate/tfidf_gating.joblib')\n",
    "\n",
    "mappings = {\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': {str(k): v for k, v in id_to_label.items()},\n",
    "    'num_labels': NUM_LABELS,\n",
    "}\n",
    "with open('data/processed/gate/label_mappings.json', 'w') as f:\n",
    "    json.dump(mappings, f, indent=2)\n",
    "\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'tfidf_threshold': TFIDF_TRASHHOLD,\n",
    "    'tfidf_min_tokens': MIN_TOKENS,\n",
    "    'use_hard_filtering': USE_HARD_FILTERING,\n",
    "    'use_soft_gating': USE_SOFT_GATING,\n",
    "    'split_sizes': {\n",
    "        'train_original': len(train_df),\n",
    "        'train_oversampled': len(train_df_oversampled),\n",
    "        'val': len(val_df),\n",
    "        'test': len(test_df)\n",
    "    }\n",
    "}\n",
    "with open('data/processed/gate/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"âœ… All data saved!\")\n",
    "print(\"\\nFiles:\")\n",
    "print(\"  - data/processed/gate/train_oversampled.csv\")\n",
    "print(\"  - data/processed/gate/tfidf_gating.joblib\")\n",
    "print(\"  - data/processed/gate/label_mappings.json\")\n",
    "print(\"  - data/processed/gate/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8988ca5",
   "metadata": {},
   "source": [
    "## Tokenization & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9029c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n",
      "  Vocab size: 50,265\n",
      "  Max length: 128\n",
      "  Padding token: <pad>\n",
      "  Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Padding token: {tokenizer.pad_token}\")\n",
    "print(f\"  Special tokens: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c29d146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test tokenizare:\n",
      "============================================================\n",
      "Text: I am so happy today!\n",
      "Tokens (6): ['I', 'Ä am', 'Ä so', 'Ä happy', 'Ä today', '!']\n",
      "Input IDs: [0, 100, 524, 98, 1372, 452, 328, 2]...\n",
      "\n",
      "Text: This makes me really angry and frustrated.\n",
      "Tokens (8): ['This', 'Ä makes', 'Ä me', 'Ä really', 'Ä angry', 'Ä and', 'Ä frustrated', '.']\n",
      "Input IDs: [0, 713, 817, 162, 269, 5800, 8, 8164, 4, 2]...\n",
      "\n",
      "Text: I'm kinda scared about what's gonna happen...\n",
      "Tokens (10): ['I', \"'m\", 'Ä kinda', 'Ä scared', 'Ä about', 'Ä what', \"'s\", 'Ä gonna', 'Ä happen', '...']\n",
      "Input IDs: [0, 100, 437, 24282, 8265, 59, 99, 18, 6908, 1369, 734, 2]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest tokenizare:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_texts = [\n",
    "    \"I am so happy today!\",\n",
    "    \"This makes me really angry and frustrated.\",\n",
    "    \"I'm kinda scared about what's gonna happen...\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    encoding = tokenizer(text, truncation=True, max_length=MAX_LENGTH)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"Input IDs: {encoding['input_ids'][:15]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ab34d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset pentru emotion classification.\n",
    "    Tokenizarea se face lazy (la __getitem__) pentru eficienÈ›Äƒ.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenizare - NU facem padding aici (se face Ã®n DataCollator)\n",
    "        encoding = self.tokenizer( text, truncation=True, max_length=self.max_length, padding = False,\n",
    "            return_tensors=None  # ReturneazÄƒ liste, nu tensori\n",
    "        )\n",
    "        \n",
    "        return { 'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask'], 'labels': label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8cbc323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Datasets created:\n",
      "   Train (oversampled): 32,172\n",
      "   Val:                 2,000\n",
      "   Test:                2,000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EmotionDataset(train_df_oversampled, tokenizer, MAX_LENGTH)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nâœ… Datasets created:\")\n",
    "print(f\"   Train (oversampled): {len(train_dataset):,}\")\n",
    "print(f\"   Val:                 {len(val_dataset):,}\")\n",
    "print(f\"   Test:                {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "531ecd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample din train_dataset:\n",
      "  input_ids (7 tokens): [0, 118, 222, 45, 619, 32386, 2]...\n",
      "  attention_mask: [1, 1, 1, 1, 1, 1, 1]...\n",
      "  label: 4 (sadness)\n",
      "\n",
      "  Decoded: <s>i did not feel humiliated</s>\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(\"\\nSample din train_dataset:\")\n",
    "print(f\"  input_ids ({len(sample['input_ids'])} tokens): {sample['input_ids'][:10]}...\")\n",
    "print(f\"  attention_mask: {sample['attention_mask'][:10]}...\")\n",
    "print(f\"  label: {sample['labels']} ({id_to_label[sample['labels']]})\")\n",
    "print(f\"\\n  Decoded: {tokenizer.decode(sample['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cac6f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataCollatorWithPadding configured!\n",
      "   - Padding: dynamic (to max length in batch)\n",
      "   - Return: PyTorch tensors\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding( tokenizer=tokenizer, padding=True, return_tensors='pt' )\n",
    "\n",
    "print(\"âœ… DataCollatorWithPadding configured!\")\n",
    "print(\"   - Padding: dynamic (to max length in batch)\")\n",
    "print(\"   - Return: PyTorch tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5357cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… DataLoaders created (batch_size=32):\n",
      "   Train: 1,006 batches\n",
      "   Val:   63 batches\n",
      "   Test:  63 batches\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32  \n",
    "\n",
    "train_loader = DataLoader( train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=0, pin_memory=True if torch.cuda.is_available() else False )\n",
    "val_loader = DataLoader( val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator, num_workers=0, pin_memory=True if torch.cuda.is_available() else False )\n",
    "test_loader = DataLoader( test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator, num_workers=0, pin_memory=True if torch.cuda.is_available() else False )\n",
    "\n",
    "print(f\"\\nâœ… DataLoaders created (batch_size={BATCH_SIZE}):\")\n",
    "print(f\"   Train: {len(train_loader):,} batches\")\n",
    "print(f\"   Val:   {len(val_loader):,} batches\")\n",
    "print(f\"   Test:  {len(test_loader):,} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "898ad67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test batch from train_loader:\n",
      "==================================================\n",
      "input_ids shape:      torch.Size([32, 54])\n",
      "attention_mask shape: torch.Size([32, 54])\n",
      "labels shape:         torch.Size([32])\n",
      "\n",
      "Decoded example:\n",
      "  i am starting to dislike the feeling of not caring about what is going to happen tomorrow...\n",
      "  Label: 3 (love)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest batch from train_loader:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"input_ids shape:      {batch['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"labels shape:         {batch['labels'].shape}\")\n",
    "print(f\"\\nDecoded example:\")\n",
    "print(f\"  {tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)[:100]}...\")\n",
    "print(f\"  Label: {batch['labels'][0].item()} ({id_to_label[batch['labels'][0].item()]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b970e2",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c39f559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All data saved!\n",
      "\n",
      "Files saved:\n",
      "  - data/processed/train_oversampled.csv\n",
      "  - data/processed/train_original.csv\n",
      "  - data/processed/val.csv\n",
      "  - data/processed/test.csv\n",
      "  - data/processed/label_mappings.json\n",
      "  - data/processed/config.json\n",
      "  - reports/oversampling_stats.json\n",
      "  - reports/class_distribution.png\n"
     ]
    }
   ],
   "source": [
    "train_df_oversampled.to_csv('data/processed/train_oversampled.csv', index=False)\n",
    "train_df.to_csv('data/processed/train_original.csv', index=False)\n",
    "val_df.to_csv('data/processed/val.csv', index=False)\n",
    "test_df.to_csv('data/processed/test.csv', index=False)\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': {str(k): v for k, v in id_to_label.items()},\n",
    "    'num_labels': NUM_LABELS,\n",
    "    'label_list': list(label_to_id.keys())\n",
    "}\n",
    "\n",
    "with open('data/processed/label_mappings.json', 'w') as f:\n",
    "    json.dump(mappings, f, indent=2)\n",
    "\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'num_labels': NUM_LABELS,\n",
    "    'slang_aware': True,\n",
    "    'split_sizes': {\n",
    "        'train_original': len(train_df),\n",
    "        'train_oversampled': len(train_df_oversampled),\n",
    "        'val': len(val_df),\n",
    "        'test': len(test_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data/processed/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "    \n",
    "print(\"âœ… All data saved!\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - data/processed/train_oversampled.csv\")\n",
    "print(\"  - data/processed/train_original.csv\")\n",
    "print(\"  - data/processed/val.csv\")\n",
    "print(\"  - data/processed/test.csv\")\n",
    "print(\"  - data/processed/label_mappings.json\")\n",
    "print(\"  - data/processed/config.json\")\n",
    "print(\"  - reports/oversampling_stats.json\")\n",
    "print(\"  - reports/class_distribution.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

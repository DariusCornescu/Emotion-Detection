{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f025c441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaborca/Documents/AI_2023-2026/Semestrul 5/NLP/Practical Project/Emotion-Detection/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate: 1.12.0\n",
      "transformers: 4.57.3\n"
     ]
    }
   ],
   "source": [
    "import accelerate, transformers\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"transformers:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f2d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch , os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "from transformers import ( RobertaTokenizerFast, RobertaModel, DataCollatorWithPadding, TrainingArguments, Trainer )\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2178a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ESA = standard self-attention + learnable feature scaling vector S ∈ R^H.\n",
    "\n",
    "    Input:\n",
    "      E_input: [B, L, H]  (RoBERTa last_hidden_state)\n",
    "      attention_mask: [B, L] (1=real, 0=pad)\n",
    "\n",
    "    Output:\n",
    "      Z_scaled: [B, L, H]\n",
    "      attn_probs: [B, L, L]  (optional, useful for debugging/visualization)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 2, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Learned positional encoding P ∈ R[max_len, H]\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "\n",
    "        # Multi-head self-attention (Transformer-style)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Learnable scaling vector S ∈ R[H]\n",
    "        self.S = nn.Parameter(torch.ones(hidden_dim))\n",
    "\n",
    "    def forward(self, E: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        E: [B, L, H]  (e.g., roberta_outputs.last_hidden_state)\n",
    "        attention_mask: [B, L] with 1=real token, 0=pad\n",
    "        \"\"\"\n",
    "        B, L, H = E.shape\n",
    "        assert H == self.hidden_dim\n",
    "        assert L <= self.max_len, f\"Sequence length {L} exceeds max_len {self.max_len}\"\n",
    "\n",
    "        # Build positions [L] and expand to [B, L] for embedding lookup\n",
    "        positions = torch.arange(L, device=E.device).unsqueeze(0).expand(B, L)  # [B, L]\n",
    "        P = self.pos_emb(positions)  # [B, L, H]\n",
    "\n",
    "        # (1) E_input = E + P\n",
    "        E_input = E + P\n",
    "\n",
    "        # Prepare key padding mask for MHA: True means \"ignore\"\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = (attention_mask == 0)  # [B, L] boolean\n",
    "\n",
    "        # (2) Standard attention\n",
    "        Z, attn_weights = self.mha(\n",
    "            E_input, E_input, E_input,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False  # returns per-head weights (closer to attention analysis)\n",
    "        )  # Z: [B, L, H]\n",
    "\n",
    "        # (3) Emotion-specific scaling: Z_scaled = Z ⊙ S\n",
    "        Z_scaled = Z * self.S  # broadcasts [H] -> [B, L, H]\n",
    "\n",
    "        # (5) Re-add positional encoding: Z_final = Z_scaled + P\n",
    "        Z_final = Z_scaled + P\n",
    "\n",
    "        return Z_final, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7cc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaESAClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout, id2label=None, label2id=None, dense_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "        cfg = self.roberta.config\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        self.esa_layer = ESALayer(\n",
    "            hidden_dim=hidden_size, \n",
    "            num_heads=cfg.num_attention_heads, \n",
    "            max_len=cfg.max_position_embeddings\n",
    "        )\n",
    "\n",
    "        self.pre_dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(2 *hidden_size, dense_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(dense_dim, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        if id2label is not None:\n",
    "            self.roberta.config.id2label = id2label\n",
    "        if label2id is not None:\n",
    "            self.roberta.config.label2id = label2id\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, inputs_embeds=None, **kwargs):\n",
    "        outputs = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        E = outputs.last_hidden_state  # [B, L, H]\n",
    "        Z_final, _ = self.esa_layer(E, attention_mask=attention_mask)\n",
    "\n",
    "        # CLS pooling\n",
    "        cls_pool = Z_final[:, 0, :] \n",
    "\n",
    "        # Mean pooling (masked)\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(Z_final)\n",
    "        sum_pool = (Z_final * mask).sum(dim=1)\n",
    "        len_pool = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        mean_pool = sum_pool / len_pool      # [B, H]\n",
    "\n",
    "        # Concatenate pooling outputs\n",
    "        pooled = torch.cat([cls_pool, mean_pool], dim=-1)  # [B, 2H]\n",
    "        X = self.dense(pooled)\n",
    "        logits = self.classifier(X)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac1a7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE -- Model încărcat pe cpu\n",
      "   Total parametri: 127,798,534\n",
      "   Parametri antrenabili: 127,798,534\n"
     ]
    }
   ],
   "source": [
    "BASELINE_CONFIGURATION = {\n",
    "    'model_name': 'roberta-base',\n",
    "    'learning_rate': 1e-5,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 10,\n",
    "    'dropout': 0.3,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_length': 128,\n",
    "    'num_labels': 6\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "id2label = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'love', 4: 'sadness', 5: 'surprise'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "os.makedirs('models/baseline_roberta', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "model = RobertaESAClassifier(\n",
    "    model_name=BASELINE_CONFIGURATION[\"model_name\"],\n",
    "    num_labels=BASELINE_CONFIGURATION[\"num_labels\"],\n",
    "    dropout=BASELINE_CONFIGURATION[\"dropout\"],\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nDONE -- Model încărcat pe {device}\")\n",
    "print(f\"   Total parametri: {total_params:,}\")\n",
    "print(f\"   Parametri antrenabili: {trainable_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c834dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    target_label=None,\n",
    "    max_length=128,\n",
    "    steps=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Integrated Gradients w.r.t. input embeddings.\n",
    "    Baseline = [CLS] + [PAD]...[PAD]\n",
    "    Returns tokens + attribution score per token.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Predict label if not provided\n",
    "    with torch.no_grad():\n",
    "        input_embeds = model.roberta.embeddings.word_embeddings(input_ids)\n",
    "        logits = model(inputs_embeds=input_embeds, attention_mask=attention_mask).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        pred = probs.argmax(dim=-1).item()\n",
    "\n",
    "    if target_label is None:\n",
    "        target_label = pred\n",
    "\n",
    "    # Build baseline ids: [CLS] + PADs\n",
    "    baseline_ids = torch.full_like(input_ids, tokenizer.pad_token_id)\n",
    "    baseline_ids[:, 0] = tokenizer.cls_token_id\n",
    "\n",
    "    # Embeddings\n",
    "    input_embeds = model.roberta.embeddings(input_ids=input_ids).detach()\n",
    "    baseline_embeds = model.roberta.embeddings(input_ids=baseline_ids).detach()\n",
    "\n",
    "    # Accumulate gradients along path\n",
    "    total_grads = torch.zeros_like(input_embeds)\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        alpha = i / steps\n",
    "        scaled = baseline_embeds + alpha * (input_embeds - baseline_embeds)\n",
    "        scaled.requires_grad_(True)\n",
    "\n",
    "        logits_i = model(inputs_embeds=scaled, attention_mask=attention_mask).logits\n",
    "        target_logit = logits_i[:, target_label].sum()\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        if scaled.grad is not None:\n",
    "            scaled.grad.zero_()\n",
    "        target_logit.backward()\n",
    "\n",
    "        total_grads += scaled.grad.detach()\n",
    "\n",
    "    avg_grads = total_grads / steps\n",
    "    ig = (input_embeds - baseline_embeds) * avg_grads  # (B, L, H)\n",
    "\n",
    "    # Token scores: sum over hidden dim\n",
    "    token_scores = ig.sum(dim=-1).squeeze(0)  # (L,)\n",
    "    token_scores = token_scores * attention_mask.squeeze(0)  # mask PADs\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).tolist())\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"pred_label\": pred,\n",
    "        \"target_label\": target_label,\n",
    "        \"probs\": probs.squeeze(0).detach().cpu(),\n",
    "        \"tokens\": tokens,\n",
    "        \"scores\": token_scores.detach().cpu(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff1eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_tokens(result, top_k=15):\n",
    "    pairs = [(t, float(s)) for t, s in zip(result[\"tokens\"], result[\"scores\"])]\n",
    "    # remove special tokens for readability\n",
    "    pairs = [(t, s) for t, s in pairs if t not in [\"<s>\", \"</s>\", \"<pad>\"]]\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    print(\"Pred:\", result[\"pred_label\"], \"Target:\", result[\"target_label\"])\n",
    "    for tok, sc in pairs_sorted[:top_k]:\n",
    "        print(f\"{tok:>12}  {sc:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c52c9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 5 Target: 5\n",
      "          'm  -0.0077\n",
      "           .  -0.0064\n",
      "      Ġangry  -0.0053\n",
      "         Ġso  -0.0050\n",
      "       Ġthat  +0.0044\n",
      "          ĠI  -0.0043\n",
      "          't  -0.0027\n",
      "      Ġright  -0.0027\n",
      "        Ġnow  +0.0025\n",
      "        Ġcan  -0.0023\n",
      "           I  +0.0023\n",
      "        Ġdid  -0.0014\n",
      "        Ġyou  -0.0003\n",
      "           .  +0.0001\n",
      "    Ġbelieve  -0.0000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(BASELINE_CONFIGURATION[\"model_name\"])\n",
    "\n",
    "text = \"I can't believe you did that. I'm so angry right now.\"\n",
    "res = integrated_gradients(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text,\n",
    "    max_length=BASELINE_CONFIGURATION[\"max_length\"],\n",
    "    steps=50\n",
    ")\n",
    "show_top_tokens(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b8b4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pred = integrated_gradients(model, tokenizer, text)\n",
    "res_anger = integrated_gradients(\n",
    "    model, tokenizer, text,\n",
    "    target_label=label2id[\"anger\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e184de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 5 Target: 5\n",
      "          'm  -0.0077\n",
      "           .  -0.0064\n",
      "      Ġangry  -0.0053\n",
      "         Ġso  -0.0050\n",
      "       Ġthat  +0.0044\n",
      "          ĠI  -0.0043\n",
      "          't  -0.0027\n",
      "      Ġright  -0.0027\n",
      "        Ġnow  +0.0025\n",
      "        Ġcan  -0.0023\n",
      "           I  +0.0023\n",
      "        Ġdid  -0.0014\n",
      "        Ġyou  -0.0003\n",
      "           .  +0.0001\n",
      "    Ġbelieve  -0.0000\n",
      "\n",
      "Attributions for target label 'anger':\n",
      "Pred: 5 Target: 0\n",
      "           .  -0.0070\n",
      "         Ġso  -0.0067\n",
      "        Ġdid  +0.0036\n",
      "      Ġright  -0.0026\n",
      "          ĠI  -0.0025\n",
      "      Ġangry  -0.0021\n",
      "           .  -0.0019\n",
      "          't  -0.0017\n",
      "       Ġthat  +0.0015\n",
      "        Ġcan  +0.0014\n",
      "        Ġyou  -0.0014\n",
      "           I  -0.0013\n",
      "    Ġbelieve  -0.0008\n",
      "          'm  -0.0003\n",
      "        Ġnow  -0.0000\n"
     ]
    }
   ],
   "source": [
    "show_top_tokens(res_pred)\n",
    "print(\"\\nAttributions for target label 'anger':\")\n",
    "show_top_tokens(res_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13af07d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: [-0.04050211 -0.06943162  0.1640822  -0.10848603 -0.07312965  0.22783612]\n",
      "probs: [0.15604365 0.15159406 0.1914679  0.14578776 0.1510345  0.20407224]\n"
     ]
    }
   ],
   "source": [
    "# check if classifier looks like random init (very small logits / unstable)\n",
    "text = \"I can't believe you did that. I'm so angry right now.\"\n",
    "enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**enc).logits.squeeze(0)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "print(\"logits:\", logits.cpu().numpy())\n",
    "print(\"probs:\", probs.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
